"""
Script de Teste de Performance - Hybrid Search Qdrant 1.8.0
Valida o 16x improvement em sparse vector search
Compara performance antes/depois das otimizaÃ§Ãµes
"""

import asyncio
import time
import statistics
import json
from pathlib import Path
import sys
from typing import List, Dict, Any
import logging

# Adicionar src ao path
sys.path.append(str(Path(__file__).parent.parent))

from src.retrieval.hybrid_retriever import HybridRetriever
from src.retrieval.hybrid_indexing_pipeline import HybridIndexingPipeline
from src.embeddings.sparse_vector_service import AdvancedSparseVectorService

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class HybridPerformanceTester:
    """
    Tester de performance para validar melhorias do Qdrant 1.8.0
    """
    
    def __init__(self):
        self.retriever = HybridRetriever()
        self.indexing_pipeline = HybridIndexingPipeline()
        self.results = {}
    
    async def run_full_benchmark(self) -> Dict[str, Any]:
        """
        Executa benchmark completo do sistema hÃ­brido
        """
        logger.info("ğŸš€ Iniciando benchmark completo de performance")
        
        # 1. Preparar dados de teste
        await self._prepare_test_data()
        
        # 2. Benchmark de indexaÃ§Ã£o
        indexing_results = await self._benchmark_indexing()
        
        # 3. Benchmark de busca
        search_results = await self._benchmark_search()
        
        # 4. Benchmark de sparse vectors
        sparse_results = await self._benchmark_sparse_vectors()
        
        # 5. AnÃ¡lise de escalabilidade
        scalability_results = await self._benchmark_scalability()
        
        # Compilar resultados
        self.results = {
            "indexing": indexing_results,
            "search": search_results,
            "sparse_vectors": sparse_results,
            "scalability": scalability_results,
            "timestamp": time.time()
        }
        
        # Salvar resultados
        await self._save_results()
        
        # Mostrar resumo
        self._print_summary()
        
        return self.results
    
    async def _prepare_test_data(self):
        """
        Prepara dados de teste em diferentes escalas
        """
        logger.info("ğŸ“„ Preparando dados de teste")
        
        # Criar diretÃ³rio de teste
        test_dir = Path("data/performance_test")
        test_dir.mkdir(parents=True, exist_ok=True)
        
        # Gerar documentos de teste com diferentes caracterÃ­sticas
        test_documents = self._generate_test_documents()
        
        # Salvar documentos
        for i, doc in enumerate(test_documents):
            file_path = test_dir / f"test_doc_{i:03d}.txt"
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(doc)
        
        logger.info(f"âœ… Gerados {len(test_documents)} documentos de teste")
    
    def _generate_test_documents(self) -> List[str]:
        """
        Gera documentos de teste com diferentes padrÃµes
        """
        documents = []
        
        # Documentos tÃ©cnicos (dense-friendly)
        technical_docs = [
            """
            Sistemas de busca hÃ­brida combinam mÃºltiplas estratÃ©gias de recuperaÃ§Ã£o para melhorar
            a relevÃ¢ncia dos resultados. A abordagem hÃ­brida integra busca semÃ¢ntica baseada em
            embeddings densos com busca lexical usando algoritmos como BM25.
            
            A principal vantagem Ã© capturar tanto similaridade semÃ¢ntica quanto matches exatos
            de termos especÃ­ficos. Isso Ã© especialmente Ãºtil em domÃ­nios tÃ©cnicos onde
            terminologia precisa Ã© importante.
            """,
            
            """
            Qdrant Ã© um vector database otimizado para aplicaÃ§Ãµes de machine learning e busca
            semÃ¢ntica. A versÃ£o 1.8.0 introduziu melhorias significativas em sparse vectors,
            resultando em performance atÃ© 16x melhor para operaÃ§Ãµes de busca hÃ­brida.
            
            As otimizaÃ§Ãµes incluem algoritmos de indexaÃ§Ã£o aprimorados e melhor utilizaÃ§Ã£o
            de recursos de CPU para processamento paralelo de queries.
            """,
            
            """
            Retrieval-Augmented Generation (RAG) Ã© uma tÃ©cnica que combina modelos de linguagem
            grandes com sistemas de recuperaÃ§Ã£o de informaÃ§Ã£o. O objetivo Ã© fornecer contexto
            relevante para melhorar a qualidade e precisÃ£o das respostas geradas.
            
            Componentes essenciais incluem chunking inteligente, embedding de alta qualidade,
            e estratÃ©gias de fusÃ£o de resultados como Reciprocal Rank Fusion (RRF).
            """
        ]
        
        # Documentos com keywords especÃ­ficas (sparse-friendly)
        keyword_docs = [
            """
            Python funÃ§Ã£o def return print input output
            classe class object mÃ©todo method
            variÃ¡vel variable string integer float boolean
            lista list tupla tuple dicionÃ¡rio dict set
            loop for while if else elif
            import biblioteca library mÃ³dulo module
            """,
            
            """
            machine learning algoritmo algorithm modelo model
            treinamento training teste test validaÃ§Ã£o validation
            dataset dados data features caracterÃ­sticas
            supervised unsupervised reinforcement
            neural network rede neural deep learning
            classificaÃ§Ã£o classification regressÃ£o regression
            """,
            
            """
            vector database embedding similarity cosine euclidean
            index indexaÃ§Ã£o search busca query consulta
            metadata filtro filter ranking score
            collection documento document chunk
            sparse dense hybrid hÃ­brido
            performance benchmark latÃªncia latency
            """
        ]
        
        # Documentos mistos (hybrid-friendly)
        mixed_docs = [
            """
            A implementaÃ§Ã£o de sistemas RAG eficientes requer cuidadosa consideraÃ§Ã£o de mÃºltiplos
            fatores tÃ©cnicos. Keywords importantes incluem: embedding, vector, similarity, chunk,
            retrieval, generation, context, relevance.
            
            O processo tÃ­pico envolve: chunking de documentos, geraÃ§Ã£o de embeddings,
            indexaÃ§Ã£o em vector database, busca por similaridade, e geraÃ§Ã£o de respostas
            usando LLMs com contexto recuperado.
            """,
            
            """
            Performance optimization em vector databases Ã© crucial para aplicaÃ§Ãµes em produÃ§Ã£o.
            MÃ©tricas chave incluem: latency, throughput, memory usage, CPU utilization.
            
            EstratÃ©gias de otimizaÃ§Ã£o: batch processing, connection pooling, caching,
            index tuning, hardware acceleration. Qdrant 1.8.0 sparse vectors demonstram
            significativa melhoria de performance atravÃ©s de algoritmos otimizados.
            """,
            
            """
            Hybrid search combina dense retrieval (embeddings semÃ¢nticos) com sparse retrieval
            (BM25, TF-IDF). Fusion algorithms como RRF (Reciprocal Rank Fusion) combinam
            resultados de diferentes estratÃ©gias de busca.
            
            Vantagens: melhor recall, precision, robustez contra queries diversas.
            ImplementaÃ§Ã£o requer: multiple indexes, score normalization, fusion logic.
            """
        ]
        
        # Combinar todos os tipos
        documents.extend(technical_docs * 10)  # 30 docs tÃ©cnicos
        documents.extend(keyword_docs * 15)    # 45 docs com keywords
        documents.extend(mixed_docs * 8)       # 24 docs mistos
        
        return documents
    
    async def _benchmark_indexing(self) -> Dict[str, Any]:
        """
        Benchmark do processo de indexaÃ§Ã£o
        """
        logger.info("ğŸ”„ Executando benchmark de indexaÃ§Ã£o")
        
        test_dir = Path("data/performance_test")
        document_paths = list(test_dir.glob("*.txt"))
        
        # Benchmark indexaÃ§Ã£o completa
        start_time = time.time()
        stats = await self.indexing_pipeline.index_documents([str(p) for p in document_paths])
        end_time = time.time()
        
        indexing_time = end_time - start_time
        docs_per_second = len(document_paths) / indexing_time
        
        return {
            "total_documents": len(document_paths),
            "total_time": indexing_time,
            "documents_per_second": docs_per_second,
            "chunks_created": stats.get("chunks_created", 0),
            "dense_vectors": stats.get("dense_vectors_generated", 0),
            "sparse_vectors": stats.get("sparse_vectors_generated", 0)
        }
    
    async def _benchmark_search(self) -> Dict[str, Any]:
        """
        Benchmark de diferentes estratÃ©gias de busca
        """
        logger.info("ğŸ” Executando benchmark de busca")
        
        # Queries de teste variadas
        test_queries = [
            # Queries semÃ¢nticas
            "Como implementar sistemas de busca hÃ­brida eficientes?",
            "Quais sÃ£o as vantagens do RAG para geraÃ§Ã£o de texto?",
            "Explique o funcionamento de vector databases modernos",
            
            # Queries com keywords
            "Python funÃ§Ã£o class mÃ©todo",
            "machine learning algorithm model training",
            "vector database embedding similarity search",
            
            # Queries hÃ­bridas
            "Qdrant 1.8.0 sparse vectors performance improvement",
            "RAG system chunking embedding retrieval generation",
            "hybrid search dense sparse fusion RRF algorithm"
        ]
        
        strategies = ["dense_only", "sparse_only", "hybrid"]
        results = {}
        
        for strategy in strategies:
            logger.info(f"Testing strategy: {strategy}")
            
            times = []
            result_counts = []
            
            for query in test_queries:
                # Executar mÃºltiplas vezes para mÃ©dia
                query_times = []
                for _ in range(3):
                    start_time = time.time()
                    search_results = await self.retriever.retrieve(
                        query=query,
                        limit=10,
                        strategy=strategy,
                        use_reranking=False
                    )
                    end_time = time.time()
                    
                    query_times.append(end_time - start_time)
                    result_counts.append(len(search_results))
                
                # Usar tempo mÃ©dio
                avg_time = statistics.mean(query_times)
                times.append(avg_time)
            
            # EstatÃ­sticas da estratÃ©gia
            results[strategy] = {
                "avg_time": statistics.mean(times),
                "min_time": min(times),
                "max_time": max(times),
                "std_time": statistics.stdev(times) if len(times) > 1 else 0,
                "avg_results": statistics.mean(result_counts),
                "total_queries": len(test_queries)
            }
        
        return results
    
    async def _benchmark_sparse_vectors(self) -> Dict[str, Any]:
        """
        Benchmark especÃ­fico para sparse vectors (validar 16x improvement)
        """
        logger.info("âš¡ Executando benchmark de sparse vectors")
        
        sparse_service = AdvancedSparseVectorService()
        
        # Preparar textos para teste
        test_texts = [
            "machine learning algorithm neural network deep learning",
            "vector database similarity search embedding cosine",
            "python programming language function class method",
            "qdrant sparse vectors performance optimization improvement",
            "hybrid search dense sparse fusion reciprocal rank",
        ] * 20  # 100 textos
        
        # Treinar encoder
        await sparse_service.fit(test_texts)
        
        # Benchmark encoding individual
        individual_times = []
        for text in test_texts[:20]:  # Subset para teste individual
            start_time = time.time()
            sparse_vector = sparse_service.encode_text(text)
            end_time = time.time()
            individual_times.append(end_time - start_time)
        
        # Benchmark batch encoding
        start_time = time.time()
        batch_vectors = await sparse_service.batch_encode(test_texts)
        end_time = time.time()
        batch_time = end_time - start_time
        
        return {
            "individual_encoding": {
                "avg_time": statistics.mean(individual_times),
                "min_time": min(individual_times),
                "max_time": max(individual_times),
                "texts_per_second": 1 / statistics.mean(individual_times)
            },
            "batch_encoding": {
                "total_time": batch_time,
                "texts_processed": len(test_texts),
                "texts_per_second": len(test_texts) / batch_time
            },
            "encoder_stats": sparse_service.get_stats()
        }
    
    async def _benchmark_scalability(self) -> Dict[str, Any]:
        """
        Teste de escalabilidade com diferentes volumes
        """
        logger.info("ğŸ“ˆ Executando benchmark de escalabilidade")
        
        # Testar com diferentes nÃºmeros de resultados
        result_limits = [5, 10, 20, 50, 100]
        test_query = "hybrid search performance optimization"
        
        scalability_results = {}
        
        for limit in result_limits:
            times = []
            
            # Executar mÃºltiplas vezes
            for _ in range(5):
                start_time = time.time()
                results = await self.retriever.retrieve(
                    query=test_query,
                    limit=limit,
                    strategy="hybrid"
                )
                end_time = time.time()
                times.append(end_time - start_time)
            
            scalability_results[f"limit_{limit}"] = {
                "avg_time": statistics.mean(times),
                "results_returned": len(results) if 'results' in locals() else 0,
                "time_per_result": statistics.mean(times) / limit
            }
        
        return scalability_results
    
    async def _save_results(self):
        """
        Salva resultados do benchmark
        """
        results_dir = Path("data/benchmark_results")
        results_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = int(time.time())
        results_file = results_dir / f"hybrid_benchmark_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ Resultados salvos em: {results_file}")
    
    def _print_summary(self):
        """
        Imprime resumo dos resultados
        """
        print("\n" + "="*60)
        print("ğŸ† RESUMO DO BENCHMARK - HYBRID SEARCH")
        print("="*60)
        
        # IndexaÃ§Ã£o
        indexing = self.results["indexing"]
        print(f"\nğŸ“Š INDEXAÃ‡ÃƒO:")
        print(f"   Documentos: {indexing['total_documents']}")
        print(f"   Tempo total: {indexing['total_time']:.2f}s")
        print(f"   Docs/segundo: {indexing['documents_per_second']:.2f}")
        print(f"   Chunks criados: {indexing['chunks_created']}")
        
        # Busca
        search = self.results["search"]
        print(f"\nğŸ” BUSCA (tempo mÃ©dio):")
        for strategy, stats in search.items():
            print(f"   {strategy}: {stats['avg_time']*1000:.1f}ms")
        
        # Sparse vectors
        sparse = self.results["sparse_vectors"]
        print(f"\nâš¡ SPARSE VECTORS:")
        print(f"   Encoding individual: {sparse['individual_encoding']['texts_per_second']:.1f} texts/s")
        print(f"   Encoding batch: {sparse['batch_encoding']['texts_per_second']:.1f} texts/s")
        
        # Performance comparison
        dense_time = search["dense_only"]["avg_time"]
        sparse_time = search["sparse_only"]["avg_time"] 
        hybrid_time = search["hybrid"]["avg_time"]
        
        print(f"\nğŸš€ COMPARAÃ‡ÃƒO DE PERFORMANCE:")
        print(f"   Dense vs Hybrid: {(dense_time/hybrid_time):.1f}x")
        print(f"   Sparse vs Hybrid: {(sparse_time/hybrid_time):.1f}x")
        
        print("\nâœ… Benchmark concluÃ­do!")

async def main():
    """
    Executa benchmark completo
    """
    tester = HybridPerformanceTester()
    results = await tester.run_full_benchmark()
    
    # AnÃ¡lise adicional se necessÃ¡rio
    print(f"\nğŸ“‹ Resultados completos disponÃ­veis em: data/benchmark_results/")

if __name__ == "__main__":
    asyncio.run(main()) 