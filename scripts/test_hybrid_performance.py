"""
Script de Teste de Performance - Hybrid Search Qdrant 1.8.0
Valida o 16x improvement em sparse vector search
Compara performance antes/depois das otimiza√ß√µes
"""

import asyncio
import time
import statistics
import json
from pathlib import Path
import sys
from typing import List, Dict, Any
import logging

# Adicionar src ao path
sys.path.append(str(Path(__file__).parent.parent))

from src.retrieval.hybrid_retriever import HybridRetriever
from src.retrieval.hybrid_indexing_pipeline import HybridIndexingPipeline
from src.embeddings.sparse_vector_service import AdvancedSparseVectorService

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class HybridPerformanceTester:
    """
    Tester de performance para validar melhorias do Qdrant 1.8.0
    """
    
    def __init__(self):
        self.retriever = HybridRetriever()
        self.indexing_pipeline = HybridIndexingPipeline()
        self.results = {}
    
    async def run_full_benchmark(self) -> Dict[str, Any]:
        """
        Executa benchmark completo do sistema h√≠brido
        """
        logger.info("üöÄ Iniciando benchmark completo de performance")
        
        # 1. Preparar dados de teste
        await self._prepare_test_data()
        
        # 2. Benchmark de indexa√ß√£o
        indexing_results = await self._benchmark_indexing()
        
        # 3. Benchmark de busca
        search_results = await self._benchmark_search()
        
        # 4. Benchmark de sparse vectors
        sparse_results = await self._benchmark_sparse_vectors()
        
        # 5. An√°lise de escalabilidade
        scalability_results = await self._benchmark_scalability()
        
        # Compilar resultados
        self.results = {
            "indexing": indexing_results,
            "search": search_results,
            "sparse_vectors": sparse_results,
            "scalability": scalability_results,
            "timestamp": time.time()
        }
        
        # Salvar resultados
        await self._save_results()
        
        # Mostrar resumo
        self._print_summary()
        
        return self.results
    
    async def _prepare_test_data(self):
        """
        Prepara dados de teste em diferentes escalas
        """
        logger.info("üìÑ Preparando dados de teste")
        
        # Criar diret√≥rio de teste
        test_dir = Path("data/performance_test")
        test_dir.mkdir(parents=True, exist_ok=True)
        
        # Gerar documentos de teste com diferentes caracter√≠sticas
        test_documents = self._generate_test_documents()
        
        # Salvar documentos
        for i, doc in enumerate(test_documents):
            file_path = test_dir / f"test_doc_{i:03d}.txt"
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(doc)
        
        logger.info(f"‚úÖ Gerados {len(test_documents)} documentos de teste")
    
    def _generate_test_documents(self) -> List[str]:
        """
        Gera documentos de teste com diferentes padr√µes
        """
        documents = []
        
        # Documentos t√©cnicos (dense-friendly)
        technical_docs = [
            """
            Sistemas de busca h√≠brida combinam m√∫ltiplas estrat√©gias de recupera√ß√£o para melhorar
            a relev√¢ncia dos resultados. A abordagem h√≠brida integra busca sem√¢ntica baseada em
            embeddings densos com busca lexical usando algoritmos como BM25.
            
            A principal vantagem √© capturar tanto similaridade sem√¢ntica quanto matches exatos
            de termos espec√≠ficos. Isso √© especialmente √∫til em dom√≠nios t√©cnicos onde
            terminologia precisa √© importante.
            """,
            
            """
            Qdrant √© um vector database otimizado para aplica√ß√µes de machine learning e busca
            sem√¢ntica. A vers√£o 1.8.0 introduziu melhorias significativas em sparse vectors,
            resultando em performance at√© 16x melhor para opera√ß√µes de busca h√≠brida.
            
            As otimiza√ß√µes incluem algoritmos de indexa√ß√£o aprimorados e melhor utiliza√ß√£o
            de recursos de CPU para processamento paralelo de queries.
            """,
            
            """
            Retrieval-Augmented Generation (RAG) √© uma t√©cnica que combina modelos de linguagem
            grandes com sistemas de recupera√ß√£o de informa√ß√£o. O objetivo √© fornecer contexto
            relevante para melhorar a qualidade e precis√£o das respostas geradas.
            
            Componentes essenciais incluem chunking inteligente, embedding de alta qualidade,
            e estrat√©gias de fus√£o de resultados como Reciprocal Rank Fusion (RRF).
            """
        ]
        
        # Documentos com keywords espec√≠ficas (sparse-friendly)
        keyword_docs = [
            """
            Python fun√ß√£o def return print input output
            classe class object m√©todo method
            vari√°vel variable string integer float boolean
            lista list tupla tuple dicion√°rio dict set
            loop for while if else elif
            import biblioteca library m√≥dulo module
            """,
            
            """
            machine learning algoritmo algorithm modelo model
            treinamento training teste test valida√ß√£o validation
            dataset dados data features caracter√≠sticas
            supervised unsupervised reinforcement
            neural network rede neural deep learning
            classifica√ß√£o classification regress√£o regression
            """,
            
            """
            vector database embedding similarity cosine euclidean
            index indexa√ß√£o search busca query consulta
            metadata filtro filter ranking score
            collection documento document chunk
            sparse dense hybrid h√≠brido
            performance benchmark lat√™ncia latency
            """
        ]
        
        # Documentos mistos (hybrid-friendly)
        mixed_docs = [
            """
            A implementa√ß√£o de sistemas RAG eficientes requer cuidadosa considera√ß√£o de m√∫ltiplos
            fatores t√©cnicos. Keywords importantes incluem: embedding, vector, similarity, chunk,
            retrieval, generation, context, relevance.
            
            O processo t√≠pico envolve: chunking de documentos, gera√ß√£o de embeddings,
            indexa√ß√£o em vector database, busca por similaridade, e gera√ß√£o de respostas
            usando LLMs com contexto recuperado.
            """,
            
            """
            Performance optimization em vector databases √© crucial para aplica√ß√µes em produ√ß√£o.
            M√©tricas chave incluem: latency, throughput, memory usage, CPU utilization.
            
            Estrat√©gias de otimiza√ß√£o: batch processing, connection pooling, caching,
            index tuning, hardware acceleration. Qdrant 1.8.0 sparse vectors demonstram
            significativa melhoria de performance atrav√©s de algoritmos otimizados.
            """,
            
            """
            Hybrid search combina dense retrieval (embeddings sem√¢nticos) com sparse retrieval
            (BM25, TF-IDF). Fusion algorithms como RRF (Reciprocal Rank Fusion) combinam
            resultados de diferentes estrat√©gias de busca.
            
            Vantagens: melhor recall, precision, robustez contra queries diversas.
            Implementa√ß√£o requer: multiple indexes, score normalization, fusion logic.
            """
        ]
        
        # Combinar todos os tipos
        documents.extend(technical_docs * 10)  # 30 docs t√©cnicos
        documents.extend(keyword_docs * 15)    # 45 docs com keywords
        documents.extend(mixed_docs * 8)       # 24 docs mistos
        
        return documents
    
    async def _benchmark_indexing(self) -> Dict[str, Any]:
        """
        Benchmark do processo de indexa√ß√£o
        """
        logger.info("üîÑ Executando benchmark de indexa√ß√£o")
        
        test_dir = Path("data/performance_test")
        document_paths = list(test_dir.glob("*.txt"))
        
        # Benchmark indexa√ß√£o completa
        start_time = time.time()
        stats = await self.indexing_pipeline.index_documents([str(p) for p in document_paths])
        end_time = time.time()
        
        indexing_time = end_time - start_time
        docs_per_second = len(document_paths) / indexing_time
        
        return {
            "total_documents": len(document_paths),
            "total_time": indexing_time,
            "documents_per_second": docs_per_second,
            "chunks_created": stats.get("chunks_created", 0),
            "dense_vectors": stats.get("dense_vectors_generated", 0),
            "sparse_vectors": stats.get("sparse_vectors_generated", 0)
        }
    
    async def _benchmark_search(self) -> Dict[str, Any]:
        """
        Benchmark de diferentes estrat√©gias de busca
        """
        logger.info("üîç Executando benchmark de busca")
        
        # Queries de teste variadas
        test_queries = [
            # Queries sem√¢nticas
            "Como implementar sistemas de busca h√≠brida eficientes?",
            "Quais s√£o as vantagens do RAG para gera√ß√£o de texto?",
            "Explique o funcionamento de vector databases modernos",
            
            # Queries com keywords
            "Python fun√ß√£o class m√©todo",
            "machine learning algorithm model training",
            "vector database embedding similarity search",
            
            # Queries h√≠bridas
            "Qdrant 1.8.0 sparse vectors performance improvement",
            "RAG system chunking embedding retrieval generation",
            "hybrid search dense sparse fusion RRF algorithm"
        ]
        
        strategies = ["dense_only", "sparse_only", "hybrid"]
        results = {}
        
        for strategy in strategies:
            logger.info(f"Testing strategy: {strategy}")
            
            times = []
            result_counts = []
            
            for query in test_queries:
                # Executar m√∫ltiplas vezes para m√©dia
                query_times = []
                for _ in range(3):
                    start_time = time.time()
                    search_results = await self.retriever.retrieve(
                        query=query,
                        limit=10,
                        strategy=strategy,
                        use_reranking=False
                    )
                    end_time = time.time()
                    
                    query_times.append(end_time - start_time)
                    result_counts.append(len(search_results))
                
                # Usar tempo m√©dio
                avg_time = statistics.mean(query_times)
                times.append(avg_time)
            
            # Estat√≠sticas da estrat√©gia
            results[strategy] = {
                "avg_time": statistics.mean(times),
                "min_time": min(times),
                "max_time": max(times),
                "std_time": statistics.stdev(times) if len(times) > 1 else 0,
                "avg_results": statistics.mean(result_counts),
                "total_queries": len(test_queries)
            }
        
        return results
    
    async def _benchmark_sparse_vectors(self) -> Dict[str, Any]:
        """
        Benchmark espec√≠fico para sparse vectors (validar 16x improvement)
        """
        logger.info("‚ö° Executando benchmark de sparse vectors")
        
        sparse_service = AdvancedSparseVectorService()
        
        # Preparar textos para teste
        test_texts = [
            "machine learning algorithm neural network deep learning",
            "vector database similarity search embedding cosine",
            "python programming language function class method",
            "qdrant sparse vectors performance optimization improvement",
            "hybrid search dense sparse fusion reciprocal rank",
        ] * 20  # 100 textos
        
        # Treinar encoder
        await sparse_service.fit(test_texts)
        
        # Benchmark encoding individual
        individual_times = []
        for text in test_texts[:20]:  # Subset para teste individual
            start_time = time.time()
            sparse_vector = sparse_service.encode_text(text)
            end_time = time.time()
            individual_times.append(end_time - start_time)
        
        # Benchmark batch encoding
        start_time = time.time()
        batch_vectors = await sparse_service.batch_encode(test_texts)
        end_time = time.time()
        batch_time = end_time - start_time
        
        return {
            "individual_encoding": {
                "avg_time": statistics.mean(individual_times),
                "min_time": min(individual_times),
                "max_time": max(individual_times),
                "texts_per_second": 1 / statistics.mean(individual_times)
            },
            "batch_encoding": {
                "total_time": batch_time,
                "texts_processed": len(test_texts),
                "texts_per_second": len(test_texts) / batch_time
            },
            "encoder_stats": sparse_service.get_stats()
        }
    
    async def _benchmark_scalability(self) -> Dict[str, Any]:
        """
        Teste de escalabilidade com diferentes volumes
        """
        logger.info("üìà Executando benchmark de escalabilidade")
        
        # Testar com diferentes n√∫meros de resultados
        result_limits = [5, 10, 20, 50, 100]
        test_query = "hybrid search performance optimization"
        
        scalability_results = {}
        
        for limit in result_limits:
            times = []
            
            # Executar m√∫ltiplas vezes
            for _ in range(5):
                start_time = time.time()
                results = await self.retriever.retrieve(
                    query=test_query,
                    limit=limit,
                    strategy="hybrid"
                )
                end_time = time.time()
                times.append(end_time - start_time)
            
            scalability_results[f"limit_{limit}"] = {
                "avg_time": statistics.mean(times),
                "results_returned": len(results) if 'results' in locals() else 0,
                "time_per_result": statistics.mean(times) / limit
            }
        
        return scalability_results
    
    async def _save_results(self):
        """
        Salva resultados do benchmark
        """
        results_dir = Path("data/benchmark_results")
        results_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = int(time.time())
        results_file = results_dir / f"hybrid_benchmark_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üíæ Resultados salvos em: {results_file}")
    
    def _print_summary(self):
        """
        Imprime resumo dos resultados
        """
        print("\n" + "="*60)
        print("üèÜ RESUMO DO BENCHMARK - HYBRID SEARCH")
        print("="*60)
        
        # Indexa√ß√£o
        indexing = self.results["indexing"]
        print(f"\nüìä INDEXA√á√ÉO:")
        print(f"   Documentos: {indexing['total_documents']}")
        print(f"   Tempo total: {indexing['total_time']:.2f}s")
        print(f"   Docs/segundo: {indexing['documents_per_second']:.2f}")
        print(f"   Chunks criados: {indexing['chunks_created']}")
        
        # Busca
        search = self.results["search"]
        print(f"\nüîç BUSCA (tempo m√©dio):")
        for strategy, stats in search.items():
            print(f"   {strategy}: {stats['avg_time']*1000:.1f}ms")
        
        # Sparse vectors
        sparse = self.results["sparse_vectors"]
        print(f"\n‚ö° SPARSE VECTORS:")
        print(f"   Encoding individual: {sparse['individual_encoding']['texts_per_second']:.1f} texts/s")
        print(f"   Encoding batch: {sparse['batch_encoding']['texts_per_second']:.1f} texts/s")
        
        # Performance comparison
        dense_time = search["dense_only"]["avg_time"]
        sparse_time = search["sparse_only"]["avg_time"] 
        hybrid_time = search["hybrid"]["avg_time"]
        
        print(f"\nüöÄ COMPARA√á√ÉO DE PERFORMANCE:")
        print(f"   Dense vs Hybrid: {(dense_time/hybrid_time):.1f}x")
        print(f"   Sparse vs Hybrid: {(sparse_time/hybrid_time):.1f}x")
        
        print("\n‚úÖ Benchmark conclu√≠do!")

async def main():
    """
    Executa benchmark completo
    """
    tester = HybridPerformanceTester()
    results = await tester.run_full_benchmark()
    
    # An√°lise adicional se necess√°rio
    print(f"\nüìã Resultados completos dispon√≠veis em: data/benchmark_results/")

if __name__ == "__main__":
    asyncio.run(main()) 