#!/usr/bin/env python3
"""
Script de Teste: HyDE + RAGAS
Valida que as implementaÃ§Ãµes HyDE e RAGAS estÃ£o funcionando corretamente
"""

import asyncio
import logging
import sys
import time
from pathlib import Path

# Adicionar src ao path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

async def test_hyde_enhancer():
    """Testa o HyDE Enhancer"""
    print("\nğŸ” Testando HyDE Enhancer...")
    
    try:
        from src.retrieval.hyde_enhancer import HyDEEnhancer
        
        # Inicializar HyDE
        hyde = HyDEEnhancer()
        
        # Teste bÃ¡sico
        test_query = "O que Ã© machine learning?"
        print(f"Query de teste: {test_query}")
        
        # Processar com HyDE
        start_time = time.time()
        result = await hyde.enhance_query(test_query)
        processing_time = time.time() - start_time
        
        # Verificar resultados
        print(f"âœ… HyDE processado em {processing_time:.3f}s")
        print(f"ğŸ“„ Documentos hipotÃ©ticos gerados: {len(result.hypothetical_documents)}")
        print(f"ğŸ¯ ConfianÃ§a: {result.confidence_score:.3f}")
        print(f"ğŸ” Embedding shape: {result.enhanced_embedding.shape}")
        
        if result.hypothetical_documents:
            print(f"ğŸ“– Exemplo de documento hipotÃ©tico:")
            print(f"   '{result.hypothetical_documents[0][:100]}...'")
        
        # Testar mÃ©tricas
        metrics = hyde.get_metrics()
        print(f"ğŸ“Š MÃ©tricas HyDE: {metrics}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erro no teste HyDE: {e}")
        return False

async def test_rag_evaluator():
    """Testa o RAG Evaluator"""
    print("\nğŸ“Š Testando RAG Evaluator...")
    
    try:
        from src.monitoring.rag_evaluator import RAGEvaluator, RAGTestCase
        
        # Inicializar avaliador
        evaluator = RAGEvaluator()
        
        # Caso de teste simples
        test_case = RAGTestCase(
            question="O que Ã© inteligÃªncia artificial?",
            contexts=[
                "InteligÃªncia artificial Ã© uma Ã¡rea da ciÃªncia da computaÃ§Ã£o que busca criar sistemas capazes de realizar tarefas que normalmente requerem inteligÃªncia humana.",
                "IA pode incluir aprendizado de mÃ¡quina, processamento de linguagem natural e visÃ£o computacional."
            ],
            answer="InteligÃªncia artificial Ã© uma Ã¡rea da ciÃªncia da computaÃ§Ã£o focada em criar sistemas que podem realizar tarefas que tradicionalmente requerem inteligÃªncia humana, como aprendizado e reconhecimento de padrÃµes.",
            ground_truth="IA Ã© um campo da computaÃ§Ã£o que desenvolve sistemas inteligentes capazes de simular capacidades humanas."
        )
        
        print(f"Caso de teste: {test_case.question}")
        
        # Avaliar
        start_time = time.time()
        result = await evaluator.evaluate_single(test_case)
        evaluation_time = time.time() - start_time
        
        # Verificar resultados
        print(f"âœ… AvaliaÃ§Ã£o concluÃ­da em {evaluation_time:.3f}s")
        print(f"ğŸ“ˆ Faithfulness: {result.faithfulness:.3f}")
        print(f"ğŸ“ˆ Answer Relevancy: {result.answer_relevancy:.3f}")
        print(f"ğŸ“ˆ Context Precision: {result.context_precision:.3f}")
        if result.context_recall:
            print(f"ğŸ“ˆ Context Recall: {result.context_recall:.3f}")
        print(f"ğŸ¯ Score Geral: {result.overall_score:.3f}")
        
        # Testar resumo
        summary = evaluator.get_evaluation_summary()
        print(f"ğŸ“Š Resumo: {summary.get('total_evaluations', 0)} avaliaÃ§Ãµes")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erro no teste RAGAS: {e}")
        return False

async def test_integration():
    """Testa integraÃ§Ã£o com sistema RAG existente"""
    print("\nğŸ”— Testando integraÃ§Ã£o com sistema RAG...")
    
    try:
        # Importar componentes do sistema
        from src.retrieval.hybrid_retriever import HybridRetriever
        
        # Testar HybridRetriever com HyDE
        retriever = HybridRetriever()
        
        test_query = "Como funciona deep learning?"
        print(f"Query de teste: {test_query}")
        
        # Testar busca com HyDE
        start_time = time.time()
        results = await retriever.retrieve(
            query=test_query,
            use_hyde=True,
            limit=5
        )
        retrieval_time = time.time() - start_time
        
        print(f"âœ… Busca hÃ­brida com HyDE em {retrieval_time:.3f}s")
        print(f"ğŸ“„ Resultados encontrados: {len(results)}")
        
        # Verificar mÃ©tricas do retriever
        metrics = retriever.get_metrics()
        hyde_time = metrics.get("hyde_time", 0)
        hyde_success = metrics.get("hyde_success_count", 0)
        
        print(f"â±ï¸  Tempo HyDE acumulado: {hyde_time:.3f}s")
        print(f"âœ… HyDE sucessos: {hyde_success}")
        
        # Verificar metadados HyDE nos resultados
        hyde_enhanced_count = 0
        for result in results:
            if result.metadata.get("hyde_enhanced"):
                hyde_enhanced_count += 1
        
        print(f"ğŸ” Resultados com HyDE: {hyde_enhanced_count}/{len(results)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erro no teste de integraÃ§Ã£o: {e}")
        return False

async def test_configuration():
    """Testa configuraÃ§Ãµes do sistema"""
    print("\nâš™ï¸ Testando configuraÃ§Ãµes...")
    
    try:
        import yaml
        
        # Verificar config.yaml
        config_path = Path("config/config.yaml")
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # Verificar seÃ§Ãµes HyDE e avaliaÃ§Ã£o
            hyde_config = config.get("hyde")
            eval_config = config.get("evaluation")
            
            if hyde_config:
                print(f"âœ… ConfiguraÃ§Ã£o HyDE encontrada:")
                print(f"   Enabled: {hyde_config.get('enabled')}")
                print(f"   Docs por query: {hyde_config.get('num_hypothetical_docs')}")
                print(f"   Modelo: {hyde_config.get('model')}")
            else:
                print(f"âš ï¸  ConfiguraÃ§Ã£o HyDE nÃ£o encontrada")
            
            if eval_config:
                print(f"âœ… ConfiguraÃ§Ã£o de avaliaÃ§Ã£o encontrada:")
                print(f"   Enabled: {eval_config.get('enabled')}")
                print(f"   Modelo: {eval_config.get('model')}")
                print(f"   MÃ©tricas: {list(eval_config.get('metrics', {}).keys())}")
            else:
                print(f"âš ï¸  ConfiguraÃ§Ã£o de avaliaÃ§Ã£o nÃ£o encontrada")
            
            # Verificar retrieval config
            retrieval_config = config.get("retrieval", {})
            use_hyde = retrieval_config.get("use_hyde", False)
            print(f"ğŸ” HyDE habilitado no retrieval: {use_hyde}")
            
        else:
            print(f"âš ï¸  Arquivo config.yaml nÃ£o encontrado em {config_path}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erro no teste de configuraÃ§Ã£o: {e}")
        return False

async def main():
    """FunÃ§Ã£o principal de teste"""
    print("ğŸ§ª TESTE COMPLETO: HyDE + RAGAS")
    print("="*50)
    
    tests = [
        ("ConfiguraÃ§Ãµes", test_configuration),
        ("HyDE Enhancer", test_hyde_enhancer),
        ("RAG Evaluator", test_rag_evaluator),
        ("IntegraÃ§Ã£o Sistema", test_integration)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        
        try:
            success = await test_func()
            results.append((test_name, success))
            
            if success:
                print(f"âœ… {test_name}: PASSOU")
            else:
                print(f"âŒ {test_name}: FALHOU")
                
        except Exception as e:
            print(f"âŒ {test_name}: ERRO - {e}")
            results.append((test_name, False))
    
    # Resumo final
    print(f"\n{'='*50}")
    print("ğŸ“Š RESUMO DOS TESTES")
    print(f"{'='*50}")
    
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    for test_name, success in results:
        status = "âœ… PASSOU" if success else "âŒ FALHOU"
        print(f"{test_name:.<30} {status}")
    
    print(f"\nğŸ¯ Resultado: {passed}/{total} testes passaram")
    
    if passed == total:
        print("ğŸ‰ Todos os testes passaram! HyDE + RAGAS implementados com sucesso!")
        return 0
    else:
        print("âš ï¸  Alguns testes falharam. Verifique os logs acima.")
        return 1

if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\nâ¹ï¸  Testes interrompidos pelo usuÃ¡rio")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ Erro fatal: {e}")
        sys.exit(1) 