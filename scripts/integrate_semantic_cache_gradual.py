#!/usr/bin/env python3
"""
Script de IntegraÃ§Ã£o Gradual - Semantic Caching
Integra o sistema de cache semÃ¢ntico gradualmente com o pipeline RAG existente
"""

import asyncio
import logging
import time
import json
from pathlib import Path
from typing import Dict, Any, Optional
import sys
import os

# Setup de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SemanticCacheIntegrator:
    """Classe para integraÃ§Ã£o gradual do cache semÃ¢ntico"""
    
    def __init__(self):
        self.integration_phases = [
            "validation",
            "parallel_testing", 
            "gradual_rollout",
            "full_integration"
        ]
        self.current_phase = None
        self.metrics = {}
        
    async def run_integration(self):
        """Executa integraÃ§Ã£o completa passo a passo"""
        
        print("ğŸš€ === INTEGRAÃ‡ÃƒO GRADUAL SEMANTIC CACHING === ğŸš€\n")
        
        try:
            # Fase 1: ValidaÃ§Ã£o do ambiente
            await self._phase_1_validation()
            
            # Fase 2: Teste paralelo
            await self._phase_2_parallel_testing()
            
            # Fase 3: Rollout gradual
            await self._phase_3_gradual_rollout()
            
            # Fase 4: IntegraÃ§Ã£o completa
            await self._phase_4_full_integration()
            
            # RelatÃ³rio final
            await self._generate_final_report()
            
        except Exception as e:
            logger.error(f"Erro na integraÃ§Ã£o: {e}")
            print(f"âŒ IntegraÃ§Ã£o falhou: {e}")
            return False
        
        print("\nğŸ‰ === INTEGRAÃ‡ÃƒO SEMANTIC CACHING CONCLUÃDA === ğŸ‰")
        return True
    
    async def _phase_1_validation(self):
        """FASE 1: ValidaÃ§Ã£o do ambiente e dependÃªncias"""
        
        print("ğŸ“‹ FASE 1: ValidaÃ§Ã£o do Ambiente")
        print("-" * 50)
        
        self.current_phase = "validation"
        phase_metrics = {
            "start_time": time.time(),
            "checks": {},
            "errors": []
        }
        
        # Check 1: DependÃªncias Python
        print("ğŸ” Verificando dependÃªncias Python...")
        try:
            import numpy as np
            phase_metrics["checks"]["numpy"] = "âœ… OK"
            print(f"   âœ… NumPy: {np.__version__}")
        except ImportError:
            error = "NumPy nÃ£o encontrado"
            phase_metrics["checks"]["numpy"] = f"âŒ {error}"
            phase_metrics["errors"].append(error)
            print(f"   âŒ NumPy: Execute 'pip install numpy'")
        
        try:
            import openai
            phase_metrics["checks"]["openai"] = "âœ… OK"
            print(f"   âœ… OpenAI: {openai.__version__}")
        except ImportError:
            error = "OpenAI nÃ£o encontrado"
            phase_metrics["checks"]["openai"] = f"âŒ {error}"
            phase_metrics["errors"].append(error)
            print(f"   âŒ OpenAI: Execute 'pip install openai'")
        
        # Check 2: Estrutura de arquivos
        print("\nğŸ—‚ï¸ Verificando estrutura de arquivos...")
        
        required_files = [
            "src/cache/semantic_cache_integration.py",
            "src/rag_pipeline_advanced.py",
            "config/"
        ]
        
        for file_path in required_files:
            if Path(file_path).exists():
                phase_metrics["checks"][file_path] = "âœ… OK"
                print(f"   âœ… {file_path}")
            else:
                error = f"Arquivo necessÃ¡rio nÃ£o encontrado: {file_path}"
                phase_metrics["checks"][file_path] = f"âŒ {error}"
                phase_metrics["errors"].append(error)
                print(f"   âŒ {file_path}")
        
        # Check 3: VariÃ¡veis de ambiente
        print("\nğŸ”‘ Verificando variÃ¡veis de ambiente...")
        
        env_vars = ["OPENAI_API_KEY"]
        for var in env_vars:
            if os.getenv(var):
                phase_metrics["checks"][var] = "âœ… OK"
                print(f"   âœ… {var}: Configurado")
            else:
                error = f"VariÃ¡vel de ambiente nÃ£o configurada: {var}"
                phase_metrics["checks"][var] = f"âš ï¸ {error}"
                print(f"   âš ï¸ {var}: NÃ£o configurado (fallback serÃ¡ usado)")
        
        # Check 4: Storage
        print("\nğŸ’¾ Verificando diretÃ³rio de storage...")
        storage_path = Path("storage")
        if not storage_path.exists():
            storage_path.mkdir(parents=True, exist_ok=True)
            print("   âœ… DiretÃ³rio 'storage' criado")
        else:
            print("   âœ… DiretÃ³rio 'storage' existe")
        
        phase_metrics["checks"]["storage"] = "âœ… OK"
        
        # Salvar mÃ©tricas da fase
        phase_metrics["duration"] = time.time() - phase_metrics["start_time"]
        self.metrics["phase_1"] = phase_metrics
        
        # Resultado da validaÃ§Ã£o
        if phase_metrics["errors"]:
            print(f"\nâš ï¸ Encontrados {len(phase_metrics['errors'])} problemas:")
            for error in phase_metrics["errors"]:
                print(f"   â€¢ {error}")
            print("\nContinuando com fallbacks quando possÃ­vel...")
        else:
            print("\nâœ… Todas as validaÃ§Ãµes passaram!")
        
        print()
        
    async def _phase_2_parallel_testing(self):
        """FASE 2: Teste paralelo sem impacto no sistema existente"""
        
        print("ğŸ§ª FASE 2: Teste Paralelo (Sem Impacto)")
        print("-" * 50)
        
        self.current_phase = "parallel_testing"
        phase_metrics = {
            "start_time": time.time(),
            "tests": {},
            "performance": {}
        }
        
        # Importar sistema de cache
        try:
            from src.cache.semantic_cache_integration import create_integrated_cache_system
            print("âœ… MÃ³dulo de cache semÃ¢ntico importado")
        except ImportError as e:
            print(f"âŒ Erro ao importar cache semÃ¢ntico: {e}")
            return
        
        # Teste 1: CriaÃ§Ã£o do sistema
        print("\nğŸ”§ Testando criaÃ§Ã£o do sistema integrado...")
        try:
            config = {
                "enable_semantic": True,
                "enable_traditional": True,
                "semantic_cache_config": {
                    "similarity_threshold": 0.85,
                    "adaptation_threshold": 0.75,
                    "max_memory_entries": 100,  # Pequeno para teste
                    "db_path": "storage/test_semantic_cache.db"
                }
            }
            
            cache_system = create_integrated_cache_system(config)
            phase_metrics["tests"]["system_creation"] = "âœ… OK"
            print("   âœ… Sistema de cache integrado criado")
            
        except Exception as e:
            phase_metrics["tests"]["system_creation"] = f"âŒ {e}"
            print(f"   âŒ Erro na criaÃ§Ã£o: {e}")
            return
        
        # Teste 2: OperaÃ§Ãµes bÃ¡sicas
        print("\nğŸ’¾ Testando operaÃ§Ãµes bÃ¡sicas...")
        
        test_queries = [
            {
                "query": "Como implementar autenticaÃ§Ã£o JWT em Python?",
                "response": {
                    "answer": "Para implementar JWT use PyJWT...",
                    "sources": [{"content": "JWT tutorial", "score": 0.9}],
                    "confidence": 0.95
                }
            },
            {
                "query": "FastAPI middleware para logging",
                "response": {
                    "answer": "Crie middleware com @app.middleware...",
                    "sources": [{"content": "FastAPI docs", "score": 0.85}],
                    "confidence": 0.88
                }
            }
        ]
        
        # Salvar no cache
        save_times = []
        for i, test_data in enumerate(test_queries):
            try:
                start_time = time.time()
                await cache_system.set(
                    test_data["query"], 
                    test_data["response"],
                    confidence_score=test_data["response"]["confidence"]
                )
                save_time = time.time() - start_time
                save_times.append(save_time)
                
                print(f"   âœ… Query {i+1} salva ({save_time:.3f}s)")
                
            except Exception as e:
                print(f"   âŒ Erro ao salvar query {i+1}: {e}")
                phase_metrics["tests"][f"save_query_{i+1}"] = f"âŒ {e}"
        
        phase_metrics["performance"]["avg_save_time"] = sum(save_times) / len(save_times) if save_times else 0
        
        # Teste 3: Busca semÃ¢ntica
        print("\nğŸ” Testando busca semÃ¢ntica...")
        
        semantic_tests = [
            ("JWT Python implementaÃ§Ã£o", 0),  # Similar Ã  primeira query
            ("Middleware logging FastAPI", 1),  # Similar Ã  segunda query
            ("Como conectar PostgreSQL", None)  # NÃ£o deve encontrar
        ]
        
        search_times = []
        semantic_hits = 0
        
        for test_query, expected_match in semantic_tests:
            try:
                start_time = time.time()
                cache_response = await cache_system.get(test_query)
                search_time = time.time() - start_time
                search_times.append(search_time)
                
                if cache_response.content is not None:
                    semantic_hits += 1
                    print(f"   âœ… '{test_query}' -> HIT (confidence: {cache_response.confidence:.3f}, {search_time:.3f}s)")
                else:
                    print(f"   â„¹ï¸ '{test_query}' -> MISS ({search_time:.3f}s)")
                
            except Exception as e:
                print(f"   âŒ Erro na busca '{test_query}': {e}")
        
        phase_metrics["performance"]["avg_search_time"] = sum(search_times) / len(search_times) if search_times else 0
        phase_metrics["performance"]["semantic_hit_rate"] = semantic_hits / len(semantic_tests)
        
        # Teste 4: Monitoramento
        print("\nğŸ“Š Testando sistema de monitoramento...")
        
        try:
            from src.monitoring.semantic_cache_integration import CacheMetricsCollector
            
            metrics_collector = CacheMetricsCollector(cache_system)
            await metrics_collector.start_collection()
            
            # Simular alguns eventos
            for i in range(5):
                metrics_collector.record_cache_event("semantic_hit", {
                    "similarity": 0.87,
                    "adaptation_applied": True
                })
            
            await asyncio.sleep(2)  # Aguardar coleta
            
            stats = metrics_collector.get_real_time_stats()
            print(f"   âœ… Monitoramento funcionando: {stats['requests']['total']} eventos registrados")
            
            metrics_collector.stop_collection()
            phase_metrics["tests"]["monitoring"] = "âœ… OK"
            
        except Exception as e:
            print(f"   âš ï¸ Monitoramento com problema: {e}")
            phase_metrics["tests"]["monitoring"] = f"âš ï¸ {e}"
        
        # Resultado da fase
        phase_metrics["duration"] = time.time() - phase_metrics["start_time"]
        self.metrics["phase_2"] = phase_metrics
        
        success_rate = len([v for v in phase_metrics["tests"].values() if v.startswith("âœ…")]) / len(phase_metrics["tests"])
        
        print(f"\nğŸ“Š Resultado dos testes:")
        print(f"   Taxa de sucesso: {success_rate:.1%}")
        print(f"   Tempo mÃ©dio de save: {phase_metrics['performance']['avg_save_time']:.3f}s")
        print(f"   Tempo mÃ©dio de busca: {phase_metrics['performance']['avg_search_time']:.3f}s")
        print(f"   Hit rate semÃ¢ntico: {phase_metrics['performance']['semantic_hit_rate']:.1%}")
        
        if success_rate >= 0.8:
            print("âœ… Testes paralelos bem-sucedidos!")
        else:
            print("âš ï¸ Alguns testes falharam - revisar configuraÃ§Ã£o")
        
        print()
    
    async def _phase_3_gradual_rollout(self):
        """FASE 3: Rollout gradual com percentual de trÃ¡fego"""
        
        print("ğŸ”„ FASE 3: Rollout Gradual")
        print("-" * 50)
        
        self.current_phase = "gradual_rollout"
        phase_metrics = {
            "start_time": time.time(),
            "rollout_stages": {}
        }
        
        # SimulaÃ§Ã£o de rollout gradual (em produÃ§Ã£o seria com trÃ¡fego real)
        rollout_stages = [
            {"percentage": 10, "duration": 3},
            {"percentage": 25, "duration": 3},
            {"percentage": 50, "duration": 3},
            {"percentage": 75, "duration": 3}
        ]
        
        print("ğŸ¯ Simulando rollout gradual:")
        print("   (Em produÃ§Ã£o, isso seria com trÃ¡fego real controlado)\n")
        
        for stage in rollout_stages:
            percentage = stage["percentage"]
            duration = stage["duration"]
            
            print(f"ğŸ“ˆ Stage: {percentage}% do trÃ¡fego por {duration}s")
            
            stage_metrics = {
                "start_time": time.time(),
                "requests_processed": 0,
                "cache_hits": 0,
                "errors": 0
            }
            
            # Simular requests por esse perÃ­odo
            for second in range(duration):
                # Simular requests (em produÃ§Ã£o seria traffic splitting real)
                requests_per_second = 10
                
                for request in range(requests_per_second):
                    stage_metrics["requests_processed"] += 1
                    
                    # Simular que X% do trÃ¡fego vai para semantic cache
                    if (request % 100) < percentage:
                        # Usar semantic cache
                        if request % 3 == 0:  # 33% de hit rate
                            stage_metrics["cache_hits"] += 1
                
                await asyncio.sleep(1)
                print(f"   â±ï¸ {second+1}/{duration}s - Requests: {stage_metrics['requests_processed']}")
            
            stage_metrics["duration"] = time.time() - stage_metrics["start_time"]
            stage_metrics["hit_rate"] = stage_metrics["cache_hits"] / max(stage_metrics["requests_processed"], 1)
            
            phase_metrics["rollout_stages"][f"{percentage}%"] = stage_metrics
            
            print(f"   âœ… Stage {percentage}% completo:")
            print(f"      Requests: {stage_metrics['requests_processed']}")
            print(f"      Hit rate: {stage_metrics['hit_rate']:.1%}")
            print(f"      Erros: {stage_metrics['errors']}")
            print()
        
        # AnÃ¡lise do rollout
        phase_metrics["duration"] = time.time() - phase_metrics["start_time"]
        self.metrics["phase_3"] = phase_metrics
        
        total_requests = sum(s["requests_processed"] for s in phase_metrics["rollout_stages"].values())
        total_hits = sum(s["cache_hits"] for s in phase_metrics["rollout_stages"].values())
        overall_hit_rate = total_hits / max(total_requests, 1)
        
        print(f"ğŸ“Š Resultado do rollout gradual:")
        print(f"   Total de requests processados: {total_requests:,}")
        print(f"   Hit rate geral: {overall_hit_rate:.1%}")
        print(f"   DuraÃ§Ã£o total: {phase_metrics['duration']:.1f}s")
        
        if overall_hit_rate > 0.2:  # 20% de hit rate mÃ­nimo
            print("âœ… Rollout gradual bem-sucedido!")
        else:
            print("âš ï¸ Hit rate abaixo do esperado - investigar")
        
        print()
    
    async def _phase_4_full_integration(self):
        """FASE 4: IntegraÃ§Ã£o completa com pipeline RAG"""
        
        print("ğŸ”— FASE 4: IntegraÃ§Ã£o Completa")
        print("-" * 50)
        
        self.current_phase = "full_integration"
        phase_metrics = {
            "start_time": time.time(),
            "integration_steps": {}
        }
        
        # Passo 1: Modificar pipeline RAG
        print("ğŸ”§ Criando exemplo de integraÃ§Ã£o com pipeline RAG...")
        
        integration_code = '''
# EXEMPLO DE INTEGRAÃ‡ÃƒO - Adicionar ao seu RAGPipelineAdvanced

from src.cache.semantic_cache_integration import create_integrated_cache_system
from src.monitoring.semantic_cache_integration import start_cache_monitoring
import asyncio
import time

class RAGPipelineAdvanced:
    def __init__(self, config):
        # Cache existente (manter para compatibilidade)
        self.traditional_cache = self._init_traditional_cache(config)
        
        # NOVO: Sistema integrado de cache
        self.integrated_cache = create_integrated_cache_system({
            "enable_semantic": config.get("enable_semantic_cache", True),
            "enable_traditional": True,
            "semantic_cache_config": {
                "similarity_threshold": config.get("semantic_threshold", 0.85),
                "adaptation_threshold": config.get("adaptation_threshold", 0.75)
            }
        })
        
        # Flag para habilitar cache semÃ¢ntico
        self.use_semantic_cache = config.get("use_semantic_cache", True)
        
        # Inicializar monitoramento
        if self.use_semantic_cache:
            asyncio.create_task(self._init_cache_monitoring())
    
    async def _init_cache_monitoring(self):
        """Inicializa monitoramento do cache"""
        try:
            self.metrics_collector, _ = await start_cache_monitoring(
                self.integrated_cache,
                show_dashboard=False
            )
        except Exception as e:
            logger.warning(f"Erro ao inicializar monitoramento: {e}")
    
    async def query(self, question: str, **kwargs) -> Dict[str, Any]:
        """Query principal com cache semÃ¢ntico"""
        
        start_time = time.time()
        
        # Tentar cache semÃ¢ntico primeiro
        if self.use_semantic_cache:
            cache_response = await self.integrated_cache.get(question, {
                "user_id": kwargs.get("user_id"),
                "domain": kwargs.get("domain", "general")
            })
            
            if cache_response.content is not None:
                # Hit no cache semÃ¢ntico
                processing_time = time.time() - start_time
                
                # Registrar evento no monitoramento
                if hasattr(self, 'metrics_collector'):
                    self.metrics_collector.record_cache_event("semantic_hit", {
                        "similarity": cache_response.confidence,
                        "adaptation_applied": cache_response.metadata.get("adaptation_applied", False)
                    })
                
                # Retornar resultado do cache
                return {
                    **cache_response.content,
                    "_cache_info": {
                        "hit": True,
                        "source": cache_response.source,
                        "confidence": cache_response.confidence,
                        "processing_time": processing_time
                    }
                }
        
        # Cache miss - processar query normalmente
        result = await self._process_query_traditional(question, **kwargs)
        processing_time = time.time() - start_time
        
        # Salvar resultado no cache semÃ¢ntico (background)
        if self.use_semantic_cache:
            asyncio.create_task(self._save_to_semantic_cache(
                question, result, processing_time
            ))
            
            # Registrar cache miss
            if hasattr(self, 'metrics_collector'):
                self.metrics_collector.record_cache_event("cache_miss")
        
        return result
    
    async def _save_to_semantic_cache(self, question: str, result: Dict, processing_time: float):
        """Salva resultado no cache semÃ¢ntico (async)"""
        try:
            await self.integrated_cache.set(
                query=question,
                response=result,
                confidence_score=result.get("confidence", 0.0),
                processing_time_saved=processing_time,
                tokens_saved=self._estimate_tokens_saved(result),
                source_model=result.get("model_used", "unknown")
            )
        except Exception as e:
            logger.warning(f"Erro ao salvar no cache semÃ¢ntico: {e}")
            if hasattr(self, 'metrics_collector'):
                self.metrics_collector.record_cache_event("error")
    
    def _estimate_tokens_saved(self, result: Dict) -> int:
        """Estima tokens economizados (implementar baseado no seu modelo)"""
        # Estimativa simples baseada no tamanho da resposta
        answer_length = len(result.get("answer", ""))
        return answer_length // 4  # ~4 chars por token
'''
        
        # Salvar cÃ³digo de exemplo
        integration_file = Path("integration_example.py")
        with open(integration_file, 'w', encoding='utf-8') as f:
            f.write(integration_code)
        
        print(f"   âœ… CÃ³digo de exemplo salvo: {integration_file}")
        phase_metrics["integration_steps"]["code_example"] = "âœ… OK"
        
        # Passo 2: ConfiguraÃ§Ã£o de produÃ§Ã£o
        print("\nğŸ“‹ Criando configuraÃ§Ã£o de produÃ§Ã£o...")
        
        production_config = {
            "semantic_cache": {
                "enable_semantic_cache": True,
                "use_semantic_cache": True,
                "semantic_threshold": 0.85,
                "adaptation_threshold": 0.75,
                "cache_config": {
                    "max_memory_entries": 1000,
                    "enable_redis": True,
                    "redis_url": "redis://localhost:6379",
                    "db_path": "storage/semantic_cache_prod.db"
                }
            },
            "monitoring": {
                "enable_detailed_logs": True,
                "metrics_collection_interval": 60,
                "alert_thresholds": {
                    "min_hit_rate": 0.15,
                    "max_error_rate": 0.05,
                    "min_avg_similarity": 0.70
                }
            }
        }
        
        config_file = Path("config/semantic_cache_production.yaml")
        config_file.parent.mkdir(exist_ok=True)
        
        import yaml
        with open(config_file, 'w', encoding='utf-8') as f:
            yaml.dump(production_config, f, default_flow_style=False, allow_unicode=True)
        
        print(f"   âœ… ConfiguraÃ§Ã£o de produÃ§Ã£o salva: {config_file}")
        phase_metrics["integration_steps"]["config"] = "âœ… OK"
        
        # Passo 3: Script de deploy
        print("\nğŸš€ Criando script de deploy...")
        
        deploy_script = '''#!/bin/bash
# Script de deploy do Semantic Cache

echo "ğŸš€ Deployando Semantic Cache..."

# 1. Backup da configuraÃ§Ã£o atual
cp config/config.yaml config/config_backup_$(date +%Y%m%d_%H%M%S).yaml

# 2. Ativar cache semÃ¢ntico gradualmente
echo "ğŸ“Š Habilitando cache semÃ¢ntico..."
# Adicionar use_semantic_cache: true na configuraÃ§Ã£o

# 3. Reiniciar serviÃ§os
echo "ğŸ”„ Reiniciando serviÃ§os..."
# docker-compose restart rag-app

# 4. Verificar health
echo "ğŸ¥ Verificando health..."
sleep 10
curl -f http://localhost:8000/health || exit 1

# 5. Monitorar mÃ©tricas
echo "ğŸ“ˆ Monitoramento ativo..."
echo "Dashboard disponÃ­vel em: http://localhost:8000/cache/metrics"

echo "âœ… Deploy concluÃ­do!"
'''
        
        deploy_file = Path("scripts/deploy_semantic_cache.sh")
        deploy_file.parent.mkdir(exist_ok=True)
        
        with open(deploy_file, 'w', encoding='utf-8') as f:
            f.write(deploy_script)
        
        deploy_file.chmod(0o755)  # Tornar executÃ¡vel
        
        print(f"   âœ… Script de deploy salvo: {deploy_file}")
        phase_metrics["integration_steps"]["deploy_script"] = "âœ… OK"
        
        # Resultado da fase
        phase_metrics["duration"] = time.time() - phase_metrics["start_time"]
        self.metrics["phase_4"] = phase_metrics
        
        print(f"\nğŸ¯ IntegraÃ§Ã£o completa preparada:")
        print(f"   ğŸ“ CÃ³digo de exemplo: integration_example.py")
        print(f"   âš™ï¸ ConfiguraÃ§Ã£o: config/semantic_cache_production.yaml")
        print(f"   ğŸš€ Script de deploy: scripts/deploy_semantic_cache.sh")
        
        print()
    
    async def _generate_final_report(self):
        """Gera relatÃ³rio final da integraÃ§Ã£o"""
        
        print("ğŸ“Š RELATÃ“RIO FINAL DA INTEGRAÃ‡ÃƒO")
        print("=" * 50)
        
        total_duration = sum(phase.get("duration", 0) for phase in self.metrics.values())
        
        report = {
            "integration_completed_at": time.time(),
            "total_duration_seconds": total_duration,
            "phases": self.metrics,
            "summary": {
                "validation_passed": len(self.metrics.get("phase_1", {}).get("errors", [])) == 0,
                "tests_success_rate": 0.0,
                "rollout_success": False,
                "integration_ready": True
            },
            "next_steps": [
                "1. Revisar cÃ³digo de exemplo: integration_example.py",
                "2. Ajustar configuraÃ§Ã£o: config/semantic_cache_production.yaml", 
                "3. Executar deploy: scripts/deploy_semantic_cache.sh",
                "4. Monitorar mÃ©tricas em tempo real",
                "5. Ajustar thresholds baseado em dados de produÃ§Ã£o"
            ]
        }
        
        # Calcular success rate dos testes
        phase_2 = self.metrics.get("phase_2", {})
        if "tests" in phase_2:
            success_count = len([v for v in phase_2["tests"].values() if v.startswith("âœ…")])
            total_count = len(phase_2["tests"])
            report["summary"]["tests_success_rate"] = success_count / max(total_count, 1)
        
        # Verificar sucesso do rollout
        phase_3 = self.metrics.get("phase_3", {})
        if "rollout_stages" in phase_3:
            stages = phase_3["rollout_stages"]
            if stages:
                avg_hit_rate = sum(s["hit_rate"] for s in stages.values()) / len(stages)
                report["summary"]["rollout_success"] = avg_hit_rate > 0.2
        
        # Salvar relatÃ³rio
        report_file = Path("storage/semantic_cache_integration_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # Exibir resumo
        print(f"â±ï¸ DuraÃ§Ã£o total: {total_duration:.1f}s")
        print(f"âœ… ValidaÃ§Ã£o: {'Passou' if report['summary']['validation_passed'] else 'Falhou'}")
        print(f"ğŸ§ª Testes: {report['summary']['tests_success_rate']:.1%} de sucesso")
        print(f"ğŸ”„ Rollout: {'Sucesso' if report['summary']['rollout_success'] else 'Revisar'}")
        print(f"ğŸ¯ Pronto para produÃ§Ã£o: {'Sim' if report['summary']['integration_ready'] else 'NÃ£o'}")
        
        print(f"\nğŸ“„ RelatÃ³rio completo salvo: {report_file}")
        
        print(f"\nğŸ¯ PRÃ“XIMOS PASSOS:")
        for step in report["next_steps"]:
            print(f"   {step}")


async def main():
    """FunÃ§Ã£o principal"""
    
    integrator = SemanticCacheIntegrator()
    
    try:
        success = await integrator.run_integration()
        
        if success:
            print(f"\nğŸ‰ IntegraÃ§Ã£o concluÃ­da com sucesso!")
            print(f"ğŸš€ O sistema estÃ¡ pronto para usar Semantic Caching!")
            return 0
        else:
            print(f"\nâŒ IntegraÃ§Ã£o falhou. Revise os logs e tente novamente.")
            return 1
            
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸ IntegraÃ§Ã£o interrompida pelo usuÃ¡rio")
        return 130
    except Exception as e:
        print(f"\nğŸ’¥ Erro inesperado: {e}")
        return 1


if __name__ == "__main__":
    import sys
    sys.exit(asyncio.run(main())) 