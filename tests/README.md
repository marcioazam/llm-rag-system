# DocumentaÃ§Ã£o de Testes - Sistema RAG

Este diretÃ³rio contÃ©m a suÃ­te completa de testes para o sistema RAG (Retrieval-Augmented Generation). Os testes sÃ£o organizados para garantir qualidade, robustez e manutenibilidade do cÃ³digo.

## ğŸ“ Estrutura de Testes

```
tests/
â”œâ”€â”€ conftest.py              # ConfiguraÃ§Ãµes globais e fixtures
â”œâ”€â”€ test_utils.py            # UtilitÃ¡rios de teste
â”œâ”€â”€ pytest.ini              # ConfiguraÃ§Ã£o do pytest
â”œâ”€â”€ test_rag_pipeline.py     # Testes do pipeline principal
â”œâ”€â”€ test_model_router.py     # Testes do roteador de modelos
â”œâ”€â”€ test_edge_cases.py       # Testes de casos extremos
â”œâ”€â”€ test_security_validation.py  # Testes de seguranÃ§a
â””â”€â”€ README.md               # Esta documentaÃ§Ã£o
```

## ğŸ§ª Tipos de Testes

### 1. Testes UnitÃ¡rios (`@pytest.mark.unit`)
- Testam componentes individuais isoladamente
- Usam mocks para dependÃªncias externas
- ExecuÃ§Ã£o rÃ¡pida e determinÃ­stica

### 2. Testes de IntegraÃ§Ã£o (`@pytest.mark.integration`)
- Testam interaÃ§Ã£o entre componentes
- Podem usar serviÃ§os reais em ambiente controlado
- Mais lentos mas mais realistas

### 3. Testes de Casos Extremos (`@pytest.mark.edge_case`)
- Testam comportamento em situaÃ§Ãµes limite
- Entradas invÃ¡lidas, recursos esgotados, etc.
- Garantem robustez do sistema

### 4. Testes de SeguranÃ§a (`@pytest.mark.security`)
- Validam proteÃ§Ãµes contra ataques
- Injection, XSS, path traversal, etc.
- CrÃ­ticos para sistemas em produÃ§Ã£o

### 5. Testes de Performance (`@pytest.mark.performance`)
- Medem tempo de execuÃ§Ã£o e uso de recursos
- Identificam gargalos e regressÃµes
- Incluem testes de concorrÃªncia

## ğŸš€ Como Executar os Testes

### ExecuÃ§Ã£o BÃ¡sica

```bash
# Executar todos os testes
pytest

# Executar com relatÃ³rio de cobertura
pytest --cov=src --cov-report=html

# Executar testes especÃ­ficos
pytest tests/test_rag_pipeline.py

# Executar teste especÃ­fico
pytest tests/test_rag_pipeline.py::TestRAGPipeline::test_init_with_config_file
```

### ExecuÃ§Ã£o por Marcadores

```bash
# Apenas testes unitÃ¡rios
pytest -m unit

# Apenas testes de integraÃ§Ã£o
pytest -m integration

# Apenas testes rÃ¡pidos (pular lentos)
pytest -m "not slow"

# Apenas testes de seguranÃ§a
pytest -m security

# Apenas testes crÃ­ticos
pytest -m critical
```

### OpÃ§Ãµes Customizadas

```bash
# ExecuÃ§Ã£o rÃ¡pida (pula testes lentos)
pytest --fast

# Apenas testes unitÃ¡rios
pytest --unit

# Apenas testes de integraÃ§Ã£o
pytest --integration

# Apenas testes de smoke
pytest --smoke

# Modo verboso com detalhes
pytest -v

# Parar no primeiro erro
pytest -x

# Executar em paralelo (se pytest-xdist instalado)
pytest -n auto
```

### RelatÃ³rios de Cobertura

```bash
# Gerar relatÃ³rio HTML
pytest --cov=src --cov-report=html

# Gerar relatÃ³rio XML (para CI/CD)
pytest --cov=src --cov-report=xml

# Mostrar linhas nÃ£o cobertas
pytest --cov=src --cov-report=term-missing

# Falhar se cobertura < 40%
pytest --cov=src --cov-fail-under=40
```

## ğŸ”§ ConfiguraÃ§Ã£o de Ambiente

### VariÃ¡veis de Ambiente

Os testes usam as seguintes variÃ¡veis de ambiente:

```bash
# ConfiguraÃ§Ã£o automÃ¡tica (via conftest.py)
TESTING=true
LOG_LEVEL=DEBUG
OPENAI_API_KEY=test-key-mock
QDRANT_HOST=localhost
QDRANT_PORT=6333
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=test
```

### DependÃªncias de Teste

Certifique-se de ter as dependÃªncias instaladas:

```bash
pip install pytest pytest-cov pytest-mock pytest-asyncio
pip install pytest-xdist  # Para execuÃ§Ã£o paralela (opcional)
pip install pytest-html   # Para relatÃ³rios HTML (opcional)
```

## ğŸ“Š Fixtures DisponÃ­veis

### Fixtures Globais (conftest.py)

- `setup_global_test_environment`: ConfiguraÃ§Ã£o global da sessÃ£o
- `temp_test_dir`: DiretÃ³rio temporÃ¡rio para a sessÃ£o
- `isolated_temp_dir`: DiretÃ³rio temporÃ¡rio por teste
- `clean_environment`: Limpeza de ambiente
- `performance_monitor`: Monitor de performance
- `sample_documents`: Documentos de exemplo
- `sample_queries`: Queries de exemplo
- `mock_config`: ConfiguraÃ§Ã£o mock
- `mock_openai_client`: Cliente OpenAI mockado
- `mock_qdrant_client`: Cliente Qdrant mockado
- `mock_neo4j_driver`: Driver Neo4j mockado

### Fixtures EspecÃ­ficas

- `base_config`: ConfiguraÃ§Ã£o base para RAG
- `custom_config_file`: Factory para arquivos de configuraÃ§Ã£o
- `setup_environment_and_mocks`: Mocks para pipeline RAG

## ğŸ›  UtilitÃ¡rios de Teste

### TestDataFactory

```python
from tests.test_utils import TestDataFactory

# Criar configuraÃ§Ã£o de teste
config = TestDataFactory.create_test_config()

# Criar documentos de teste
docs = TestDataFactory.create_test_documents(count=5)

# Criar resultados de busca
results = TestDataFactory.create_search_results()
```

### MockFactory

```python
from tests.test_utils import MockFactory

# Criar mocks prÃ©-configurados
embedding_mock = MockFactory.create_embedding_service_mock()
router_mock = MockFactory.create_model_router_mock()
openai_mock = MockFactory.create_openai_mock()
```

### TestValidators

```python
from tests.test_utils import TestValidators

# Validar resposta de query
TestValidators.validate_query_response(response)

# Validar processamento de documento
TestValidators.validate_document_processing(result)

# Validar resultados de busca
TestValidators.validate_search_results(results)
```

### PerformanceTestHelper

```python
from tests.test_utils import PerformanceTestHelper

# Medir tempo de execuÃ§Ã£o
with PerformanceTestHelper.measure_time() as timer:
    # cÃ³digo a ser medido
    pass

print(f"Tempo: {timer.duration}s")

# Benchmark de queries
results = PerformanceTestHelper.benchmark_queries(pipeline, queries)
```

## ğŸ“ˆ MÃ©tricas e Monitoramento

### Cobertura de CÃ³digo

- **Meta**: MÃ­nimo 40% de cobertura
- **Atual**: Verificar com `pytest --cov=src --cov-report=term`
- **RelatÃ³rios**: Gerados em `htmlcov/` e `coverage.xml`

### Performance

- Testes com duraÃ§Ã£o > 5s geram warnings
- Use `@pytest.mark.slow` para testes lentos
- Monitor automÃ¡tico via `performance_monitor` fixture

### Qualidade

- Todos os testes devem passar
- Sem warnings crÃ­ticos
- Mocks adequados para dependÃªncias externas
- Isolamento entre testes

## ğŸ› Debugging de Testes

### Logs Detalhados

```bash
# Habilitar logs detalhados
pytest -s --log-cli-level=DEBUG

# Capturar stdout/stderr
pytest -s --capture=no
```

### Debugging EspecÃ­fico

```bash
# Executar teste especÃ­fico com debugging
pytest -xvs tests/test_rag_pipeline.py::test_specific_function

# Usar pdb para debugging interativo
pytest --pdb

# Parar no primeiro erro
pytest -x
```

### Problemas Comuns

1. **Testes falhando por dependÃªncias externas**
   - Verificar se mocks estÃ£o configurados
   - Usar `@pytest.mark.external` para testes que precisam de serviÃ§os

2. **Problemas de isolamento**
   - Usar `clean_environment` fixture
   - Verificar se variÃ¡veis globais estÃ£o sendo resetadas

3. **Testes lentos**
   - Marcar com `@pytest.mark.slow`
   - Otimizar ou usar mocks mais eficientes

4. **Problemas de cobertura**
   - Verificar se todos os caminhos estÃ£o testados
   - Adicionar testes para casos extremos

## ğŸ“ Boas PrÃ¡ticas

### Escrita de Testes

1. **Nomes descritivos**: `test_should_return_error_when_config_file_not_found`
2. **Arrange-Act-Assert**: Estrutura clara dos testes
3. **Isolamento**: Cada teste deve ser independente
4. **Mocks apropriados**: Mockar dependÃªncias externas
5. **Assertions especÃ­ficas**: Verificar comportamento exato

### OrganizaÃ§Ã£o

1. **Um arquivo por mÃ³dulo**: `test_module_name.py`
2. **Classes para agrupamento**: `TestClassName`
3. **Fixtures reutilizÃ¡veis**: Evitar duplicaÃ§Ã£o
4. **DocumentaÃ§Ã£o**: Docstrings explicativas
5. **Marcadores**: Classificar tipos de teste

### Performance

1. **Testes rÃ¡pidos**: Priorizar velocidade
2. **Mocks eficientes**: Evitar operaÃ§Ãµes custosas
3. **ParalelizaÃ§Ã£o**: Usar pytest-xdist quando possÃ­vel
4. **Cleanup**: Limpar recursos apÃ³s testes
5. **Monitoramento**: Acompanhar tempo de execuÃ§Ã£o

## ğŸ”„ IntegraÃ§Ã£o ContÃ­nua

### GitHub Actions / CI

```yaml
# Exemplo de configuraÃ§Ã£o CI
- name: Run tests
  run: |
    pytest --cov=src --cov-report=xml --cov-fail-under=40
    
- name: Upload coverage
  uses: codecov/codecov-action@v1
  with:
    file: ./coverage.xml
```

### Pre-commit Hooks

```yaml
# .pre-commit-config.yaml
- repo: local
  hooks:
    - id: pytest
      name: pytest
      entry: pytest
      language: system
      pass_filenames: false
      always_run: true
```

## ğŸ“š Recursos Adicionais

- [DocumentaÃ§Ã£o do pytest](https://docs.pytest.org/)
- [pytest-cov](https://pytest-cov.readthedocs.io/)
- [Mocking com unittest.mock](https://docs.python.org/3/library/unittest.mock.html)
- [Boas prÃ¡ticas de teste](https://docs.pytest.org/en/stable/goodpractices.html)

---

**Nota**: Esta documentaÃ§Ã£o Ã© atualizada conforme novos testes sÃ£o adicionados. Para dÃºvidas ou sugestÃµes, consulte a equipe de desenvolvimento.