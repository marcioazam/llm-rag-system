# ===================================================================
# CONFIGURAÇÃO SISTEMA RAG AVANÇADO - 100% APIS EXTERNAS
# Usado pelo AdvancedRAGPipeline (sem modelos locais)
# ===================================================================

# Configuração de Provedores LLM
providers:
  openai:
    api_key: "${OPENAI_API_KEY}"
    models:
      gpt4o:
        name: "gpt-4o"
        tasks: ["complex_analysis", "architecture_design", "code_review"]
        max_tokens: 4096
        temperature: 0.7
        cost_per_1k_tokens: 0.005
        priority: 1
      gpt4o_mini:
        name: "gpt-4o-mini"
        tasks: ["code_generation", "debugging", "quick_responses"]
        max_tokens: 16384
        temperature: 0.3
        cost_per_1k_tokens: 0.00015
        priority: 2
      gpt35_turbo:
        name: "gpt-3.5-turbo"
        tasks: ["general_qa", "summaries"]
        max_tokens: 4096
        temperature: 0.5
        cost_per_1k_tokens: 0.0005
        priority: 3
        
  anthropic:
    api_key: "${ANTHROPIC_API_KEY}"
    models:
      claude35_sonnet:
        name: "claude-3-5-sonnet-20241022"
        tasks: ["technical_writing", "document_analysis", "complex_reasoning"]
        max_tokens: 8192
        temperature: 0.7
        cost_per_1k_tokens: 0.003
        priority: 1
      claude3_haiku:
        name: "claude-3-haiku-20240307"
        tasks: ["fast_responses", "simple_qa"]
        max_tokens: 4096
        temperature: 0.3
        cost_per_1k_tokens: 0.00025
        priority: 2
        
  google:
    api_key: "${GOOGLE_API_KEY}"
    models:
      gemini15_pro:
        name: "gemini-1.5-pro"
        tasks: ["long_context", "multimodal_analysis"]
        max_tokens: 8192
        temperature: 0.7
        cost_per_1k_tokens: 0.0035
        priority: 1
      gemini15_flash:
        name: "gemini-1.5-flash"
        tasks: ["fast_responses", "quick_analysis"]
        max_tokens: 8192
        temperature: 0.3
        cost_per_1k_tokens: 0.000075
        priority: 2
        
  deepseek:
    api_key: "${DEEPSEEK_API_KEY}"
    models:
      deepseek_chat:
        name: "deepseek-chat"
        tasks: ["advanced_coding", "mathematical_reasoning"]
        max_tokens: 4096
        temperature: 0.3
        cost_per_1k_tokens: 0.00014
        priority: 1
      deepseek_coder:
        name: "deepseek-coder"
        tasks: ["code_analysis", "code_generation"]
        max_tokens: 4096
        temperature: 0.1
        cost_per_1k_tokens: 0.00014
        priority: 1

# Configuração de Embeddings (100% API)
embeddings:
  primary_provider: "openai"
  providers:
    openai:
      models:
        text_embedding_3_small:
          name: "text-embedding-3-small"
          dimensions: 1536
          max_input: 8191
          cost_per_1k_tokens: 0.00002
          use_for: ["quick_embeddings", "classification"]
        text_embedding_3_large:
          name: "text-embedding-3-large"
          dimensions: 3072
          max_input: 8191
          cost_per_1k_tokens: 0.00013
          use_for: ["high_quality_search", "complex_similarity"]

# Roteamento Inteligente
routing:
  strategy: "cost_performance_optimized"
  fallback_chain:
    - "openai.gpt4o_mini"
    - "anthropic.claude3_haiku"
    - "google.gemini15_flash"
  
  task_routing:
    code_generation: "openai.gpt4o_mini"
    code_analysis: "deepseek.deepseek_coder"
    debugging: "openai.gpt4o_mini"
    architecture_design: "anthropic.claude35_sonnet"
    technical_writing: "anthropic.claude35_sonnet"
    complex_analysis: "openai.gpt4o"
    quick_responses: "google.gemini15_flash"
    long_context: "google.gemini15_pro"

# Configurações Avançadas do RAG
advanced_features:
  # Enhanced Corrective RAG com APIs reais
  enhanced_corrective_rag:
    enabled: true
    relevance_threshold: 0.75
    max_reformulation_attempts: 3
    enable_decomposition: true
    
    # Configuração de APIs para T5 Evaluator
    api_providers:
      primary: "openai"
      fallback_chain: ["openai", "anthropic", "huggingface"]
      openai:
        model: "gpt-4o-mini"
        max_tokens: 500
        temperature: 0.1
      anthropic:
        model: "claude-3-haiku-20240307"
        max_tokens: 500
        temperature: 0.1
      huggingface:
        model: "google/flan-t5-large"
        max_tokens: 500
    
    # Cache multicamada para avaliações
    cache:
      enable_l1: true  # Memória
      enable_l2: true  # Redis
      enable_l3: true  # SQLite
      l1_max_size: 1000
      redis_host: "${REDIS_HOST:-localhost}"
      redis_port: "${REDIS_PORT:-6379}"
      redis_db: "${REDIS_DB:-1}"
      redis_password: "${REDIS_PASSWORD:-}"
      sqlite_path: "cache/enhanced_rag_evaluations.db"
      default_ttl: 3600  # 1 hora
      promotion_threshold: 3
    
    # Circuit breaker para APIs
    circuit_breaker:
      failure_threshold: 3
      recovery_timeout: 60
      timeout: 30
    
    # Métricas e monitoramento
    monitoring:
      track_api_usage: true
      track_cache_performance: true
      log_evaluation_details: false  # Para debug
      alert_on_failures: true
  
  # Corrective RAG (fallback)
  corrective_rag:
    enabled: true
    relevance_threshold: 0.7
    max_reformulation_attempts: 2
    
  # Multi-Query RAG
  multi_query:
    enabled: true
    num_variations: 3
    fusion_method: "weighted_rrf"
    variation_strategies: ["specific", "general", "related"]
    
  # Adaptive Retrieval
  adaptive_retrieval:
    enabled: true
    min_k: 3
    max_k: 15
    query_types: ["definition", "list", "comparison", "implementation", "analysis"]
    
  # Enhanced GraphRAG
  graph_rag:
    enabled: true
    max_hops: 3
    community_detection: true
    min_community_size: 3
    entity_scoring: ["degree", "betweenness"]

# Otimizações de Performance
optimization:
  caching:
    enabled: true
    cache_embeddings: true
    cache_queries: true
    ttl_seconds: 3600
    max_cache_size: 10000
    
  rate_limiting:
    requests_per_minute: 60
    concurrent_requests: 10
    
  cost_control:
    daily_budget_usd: 50.0
    cost_tracking: true
    alert_threshold: 0.8

# Vector Database (Qdrant)
vectordb:
  type: "qdrant"
  host: "localhost"
  port: 6333
  collection_name: "advanced_rag_collection"
  
# Monitoramento
monitoring:
  enabled: true
  metrics_collection: true
  prometheus_port: 8001
  log_level: "INFO"

# Desenvolvimento
development:
  verbose_logging: false
  cache_disabled: false
  mock_apis: false

# RAPTOR Configuration (Integration System)
raptor_integration:
  # Estratégia de implementação
  preferred_implementation: "enhanced"  # enhanced, offline, original
  auto_fallback: true
  
  # Configurações Gerais
  chunk_size: 400
  chunk_overlap: 80
  max_levels: 4
  min_cluster_size: 2
  max_cluster_size: 50
  
  # Enhanced Implementation (Preferred)
  enhanced:
    enabled: true
    
    # Embedding Configuration
    embedding:
      provider: "sentence_transformers"  # openai, sentence_transformers, huggingface
      model: "all-MiniLM-L6-v2"
      openai_model: "text-embedding-3-small"
      cache_embeddings: true
      batch_size: 32
      
    # Clustering Configuration
    clustering:
      method: "umap_gmm"  # umap_gmm, pca_gmm, kmeans
      n_components: 8
      min_dist: 0.0
      n_neighbors: 15
      quality_threshold: 0.3
      
    # Summarization Configuration  
    summarization:
      use_llm: false  # Set to true if OpenAI API key available
      provider: "openai"  # openai, anthropic, google
      model: "gpt-4o-mini"
      max_tokens: 800
      temperature: 0.1
      fallback_to_concat: true
      
    # Cache Configuration
    cache:
      enabled: true
      type: "multi_layer"  # memory, redis, multi_layer
      ttl: 3600
      max_memory_entries: 1000
      redis_enabled: false  # Auto-detect based on REDIS_URL
      
    # Performance Configuration
    performance:
      max_workers: 4
      parallel_embeddings: true
      parallel_summarization: true
      memory_optimization: true
      batch_processing: true
      
  # Offline Implementation (Fallback)
  offline:
    enabled: true
    embedding_model: "all-MiniLM-L6-v2"
    clustering_method: "pca_gmm"
    max_levels: 3
    use_intelligent_summary: true
    
  # Original Implementation (Legacy)
  original:
    enabled: true
    embedding_model: "sentence-transformers/all-mpnet-base-v2"
    clustering_strategy: "global_local"
    retrieval_strategy: "collapsed_tree"
    umap:
      n_components: 10
      min_dist: 0.0
      local_neighbors: 10
    
  # API Keys and Authentication
  api_keys:
    openai_api_key: "${OPENAI_API_KEY}"
    anthropic_api_key: "${ANTHROPIC_API_KEY}"
    google_api_key: "${GOOGLE_API_KEY}"
    
  # Monitoring and Metrics
  monitoring:
    enabled: true
    track_implementation_usage: true
    track_fallback_events: true
    track_performance_metrics: true
    log_construction_details: false
    log_search_details: false
    
  # Development Settings
  development:
    debug_mode: false
    force_implementation: null  # enhanced, offline, original
    mock_llm_calls: false
    save_intermediate_results: false

# Legacy RAPTOR Configuration (Backward Compatibility)
raptor:
  enabled: true
  embedding_model: "sentence-transformers/all-mpnet-base-v2"
  chunk_size: 250
  chunk_overlap: 50
  clustering_strategy: "global_local"  # global_local, single_level, adaptive
  retrieval_strategy: "collapsed_tree"  # tree_traversal, collapsed_tree, hybrid
  max_levels: 5
  min_cluster_size: 2
  max_cluster_size: 100
  api_provider: "openai"
  model_name: "gpt-4o-mini"
  
  # UMAP Parameters
  umap:
    n_components: 10
    min_dist: 0.0
    local_neighbors: 10
    global_neighbors: null  # auto-calculated
    
  # Summarization Parameters
  summarization:
    max_tokens: 1000
    strategy: "hierarchical"
    
  # Retrieval Parameters
  retrieval:
    default_k: 10
    default_max_tokens: 2000
    enable_caching: true
    
  # Performance Tuning
  performance:
    batch_size: 32
    parallel_clustering: true
    memory_efficient: true 