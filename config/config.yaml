llm:
  model: llama3.1:8b-instruct-q4_K_M
  base_url: http://localhost:11434
  temperature: 0.7
  max_tokens: 2048
  routing_mode: advanced
  models:
    general:
      name: llama3.1:8b-instruct-q4_K_M
      temperature: 0.7
      max_tokens: 2048
      tasks:
      - general_explanation
      - documentation
      priority: 1
    code:
      name: codellama:7b-instruct
      temperature: 0.3
      max_tokens: 4096
      tasks:
      - code_generation
      - debugging
      priority: 1
    mistral:
      name: mistral:7b-instruct-q4_0
      temperature: 0.8
      max_tokens: 2048
      tasks:
      - architecture_design
      - system_analysis
      priority: 2
      optional: true
    sql:
      name: sqlcoder:7b-q4_0
      temperature: 0.1
      max_tokens: 1024
      tasks:
      - sql_query
      priority: 1
      optional: true
    fast:
      name: phi:2.7b
      temperature: 0.5
      max_tokens: 512
      tasks:
      - quick_snippet
      - validation
      priority: 3
      optional: true
performance:
  max_concurrent_models: 2
  cpu_threads: 6
  use_gpu: false
  model_timeout: 300
  cache_models: true
  model_cache_ttl: 3600
embeddings:
  model_name: BAAI/bge-small-en-v1.5
  device: cpu
  batch_size: 32
  cache_embeddings: true
  normalize_embeddings: true
vectordb:
  type: qdrant
  host: localhost
  port: 6333
  collection_name: rag_chunks
  persist_directory: ./data/indexes
  metadata_fields:
  - source
  - doc_type
  - language
  - complexity
chunking:
  method: advanced
  chunk_size: 512
  chunk_overlap: 50
  min_chunk_size: 100
  max_chunk_size: 1000
  adaptive_chunking:
    code:
      chunk_size: 1024
      chunk_overlap: 100
    documentation:
      chunk_size: 512
      chunk_overlap: 50
    sql:
      chunk_size: 256
      chunk_overlap: 25
# HyDE (Hypothetical Document Embeddings) Configuration
hyde:
  enabled: true
  num_hypothetical_docs: 3
  max_tokens: 400
  temperature: 0.7
  model: "gpt-3.5-turbo"
  cache_enabled: true
  embedding_strategy: "mean"  # mean, weighted, max
  prompt_template:
    system: "Você é um especialista em gerar documentos informativos e precisos."
    user: |
      Dada a pergunta abaixo, gere um parágrafo detalhado que responda à pergunta de forma completa e informativa.
      O texto deve ser factual, bem estruturado e conter informações relevantes que poderiam estar em um documento real.
      
      Pergunta: {query}
      
      Parágrafo de resposta:
  quality_threshold: 0.3

retrieval:
  top_k: 5
  similarity_threshold: 0.7
  rerank: true
  rerank_model: cross-encoder/ms-marco-MiniLM-L-6-v2
  use_hyde: false  # Habilitar HyDE por padrão
  context_aware:
    enabled: true
    boost_recent: true
    boost_factor: 1.2

# Configuração de Avaliação RAG
evaluation:
  enabled: true
  model: "gpt-3.5-turbo"
  max_tokens: 1000
  temperature: 0.1
  cache_enabled: true
  max_concurrent_evaluations: 5
  metrics:
    faithfulness:
      enabled: true
      weight: 0.3
    answer_relevancy:
      enabled: true
      weight: 0.3
    context_precision:
      enabled: true
      weight: 0.2
    context_recall:
      enabled: true
      weight: 0.2
  thresholds:
    good_score: 0.7
    acceptable_score: 0.5
  auto_evaluation:
    enabled: false
    sample_rate: 0.1  # Avaliar 10% das queries automaticamente
    min_interval_hours: 24
rag:
  fallback_to_llm: true
  min_relevance_score: 0.5
  hybrid_mode: true
  multi_model:
    enabled: true
    task_detection:
      enabled: true
      confidence_threshold: 0.7
    model_selection:
      strategy: best_available
      fallback_chain:
      - general
      - code
      - fast
    response_fusion:
      enabled: true
      method: sequential
logging:
  level: INFO
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  file: logs/rag_system.log
  max_file_size: 10MB
  backup_count: 3
  model_logging:
    log_prompts: false
    log_responses: false
    log_performance: true
api:
  host: 0.0.0.0
  port: 8000
  reload: true
  workers: 1
  cors_origins:
  - 'http://localhost:3000'
  - 'http://localhost:8080'
  - 'http://127.0.0.1:3000'
  - 'http://127.0.0.1:8080'
  rate_limit:
    enabled: true
    requests_per_minute: 10
    burst_size: 20
    per_user: true
monitoring:
  enabled: true
  check_interval: 60
  alerts:
    memory_threshold: 85
    cpu_threshold: 90
    model_load_time_threshold: 30
debug:
  verbose: false
  show_model_selection: true
  show_retrieval_scores: false
  show_chunk_details: false
  benchmark_mode: false
