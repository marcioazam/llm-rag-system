# LLM Configuration
llm:
  model: "llama3.1:8b"
  base_url: "http://localhost:11434"
  temperature: 0.7
  max_tokens: 2048

# Embedding Configuration
embeddings:
  model_name: "BAAI/bge-small-en-v1.5"
  device: "cuda"  # ou "cpu"
  batch_size: 32

# Vector Database Configuration
vectordb:
  type: "chromadb"
  persist_directory: "./data/indexes/chroma"
  collection_name: "documents"

# Chunking Configuration
chunking:
  method: "semantic"  # semantic, recursive, fixed
  chunk_size: 512
  chunk_overlap: 50
  min_chunk_size: 100
  max_chunk_size: 1000

# Retrieval Configuration
retrieval:
  top_k: 5
  similarity_threshold: 0.7
  rerank: true
  rerank_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"

rag:
  fallback_to_llm: true  # Se true, usa conhecimento do LLM quando não encontra documentos
  min_relevance_score: 0.5  # Score mínimo para considerar um documento relevante
  hybrid_mode: true  # Se true, sempre combina contexto + conhecimento do LLM
