from __future__ import annotations

import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass

from .dynamic_prompt_system import DynamicPromptSystem
from ..prompt_selector import select_prompt, classify_query

logger = logging.getLogger(__name__)


@dataclass
class PromptGenerationResult:
    """Resultado da gera√ß√£o de prompt com metadados."""
    final_prompt: str
    template_id: str
    task_type: str
    prompt_source: str  # "template" ou "dynamic"
    confidence: float
    metadata: Dict[str, Any]


class UnifiedPromptSystem:
    """
    Sistema unificado que combina:
    1. Prompt Selector - Sele√ß√£o inteligente baseada em heur√≠sticas
    2. Dynamic Prompt System - Gera√ß√£o din√¢mica com contexto e templates
    
    Fluxo:
    1. Classifica a query (bugfix, code, review, etc.)
    2. Seleciona template apropriado baseado na classifica√ß√£o
    3. Gera prompt din√¢mico combinando template + contexto + query
    4. Aplica otimiza√ß√µes baseadas no tipo de tarefa
    """

    def __init__(self):
        self.dynamic_system = DynamicPromptSystem()
        self.prompt_cache = {}  # Cache para templates processados
        
        # Configura√ß√µes por tipo de tarefa
        self.task_configs = {
            "bugfix": {
                "reasoning_required": True,
                "max_context_chunks": 3,
                "temperature": 0.3,  # Mais conservador
                "system_enhancement": "Foque em identificar a causa raiz e fornecer solu√ß√µes test√°veis."
            },
            "review": {
                "reasoning_required": True,
                "max_context_chunks": 5,
                "temperature": 0.4,
                "system_enhancement": "Analise criticamente seguindo melhores pr√°ticas de code review."
            },
            "perf": {
                "reasoning_required": True,
                "max_context_chunks": 4,
                "temperature": 0.3,
                "system_enhancement": "Identifique gargalos e sugira otimiza√ß√µes mensur√°veis."
            },
            "arch": {
                "reasoning_required": True,
                "max_context_chunks": 6,
                "temperature": 0.5,
                "system_enhancement": "Considere trade-offs, escalabilidade e manutenibilidade."
            },
            "test": {
                "reasoning_required": False,
                "max_context_chunks": 3,
                "temperature": 0.4,
                "system_enhancement": "Gere testes abrangentes seguindo pir√¢mide de testes."
            },
            "testgen": {
                "reasoning_required": False,
                "max_context_chunks": 2,
                "temperature": 0.3,
                "system_enhancement": "Crie casos de teste que cubram cen√°rios edge case."
            },
            "data_viz": {
                "reasoning_required": False,
                "max_context_chunks": 3,
                "temperature": 0.6,
                "system_enhancement": "Sugira visualiza√ß√µes que revelem insights nos dados."
            },
            "ci": {
                "reasoning_required": True,
                "max_context_chunks": 4,
                "temperature": 0.3,
                "system_enhancement": "Diagnostique problemas de CI/CD e sugira corre√ß√µes."
            },
            "geral": {
                "reasoning_required": False,
                "max_context_chunks": 5,
                "temperature": 0.5,
                "system_enhancement": "Forne√ßa resposta clara e bem estruturada."
            }
        }

    async def generate_optimal_prompt(
        self,
        query: str,
        context_chunks: List[str],
        language: str = "Portugu√™s",
        depth: str = "quick",
        force_template: Optional[str] = None
    ) -> PromptGenerationResult:
        """
        Gera prompt otimizado combinando sele√ß√£o inteligente com gera√ß√£o din√¢mica.
        
        Args:
            query: Query do usu√°rio
            context_chunks: Chunks de contexto relevantes
            language: Idioma da resposta
            depth: "quick" ou "deep" para complexidade
            force_template: For√ßar uso de template espec√≠fico
            
        Returns:
            PromptGenerationResult com prompt final e metadados
        """
        # 1. Classificar query e selecionar template
        task_type = classify_query(query)
        task_config = self.task_configs.get(task_type, self.task_configs["geral"])
        
        logger.info(f"üéØ Query classificada como: {task_type}")
        
        # 2. Selecionar template apropriado
        try:
            template_id, template_text = select_prompt(query, depth=depth)
            prompt_source = "template"
            confidence = 0.9  # Alta confian√ßa em templates selecionados
            
            logger.info(f"üìã Template selecionado: {template_id}")
            
        except Exception as e:
            logger.warning(f"Erro ao selecionar template: {e}. Usando sistema din√¢mico.")
            template_text = None
            template_id = "dynamic_fallback"
            prompt_source = "dynamic"
            confidence = 0.7

        # 3. Preparar contexto otimizado baseado no tipo de tarefa
        max_chunks = task_config["max_context_chunks"]
        optimized_chunks = context_chunks[:max_chunks] if context_chunks else []
        
        # 4. Gerar prompt base usando sistema din√¢mico
        if template_text:
            # Usar template como base e enriquecer
            final_prompt = self._merge_template_with_dynamic(
                template_text, query, optimized_chunks, task_type, language
            )
        else:
            # Fallback para sistema din√¢mico puro
            final_prompt = self.dynamic_system.generate_prompt(
                query, optimized_chunks, task_type, language
            )

        # 5. Aplicar enhancements espec√≠ficos da tarefa
        final_prompt = self._apply_task_enhancements(
            final_prompt, task_type, task_config, query
        )

        # 6. Criar resultado com metadados
        result = PromptGenerationResult(
            final_prompt=final_prompt,
            template_id=template_id,
            task_type=task_type,
            prompt_source=prompt_source,
            confidence=confidence,
            metadata={
                "language": language,
                "depth": depth,
                "context_chunks_used": len(optimized_chunks),
                "max_chunks_allowed": max_chunks,
                "temperature_suggestion": task_config["temperature"],
                "reasoning_required": task_config["reasoning_required"],
                "system_enhancement": task_config["system_enhancement"]
            }
        )

        logger.info(f"‚úÖ Prompt gerado: {len(final_prompt)} chars, confian√ßa: {confidence:.2f}")
        return result

    def _merge_template_with_dynamic(
        self,
        template_text: str,
        query: str,
        context_chunks: List[str],
        task_type: str,
        language: str
    ) -> str:
        """Combina template selecionado com sistema din√¢mico."""
        
        # Gerar prompt din√¢mico para contexto e estrutura
        dynamic_prompt = self.dynamic_system.generate_prompt(
            query, context_chunks, task_type, language
        )
        
        # Extrair partes do prompt din√¢mico
        dynamic_parts = dynamic_prompt.split("\n\n")
        system_prompt = dynamic_parts[0] if dynamic_parts else ""
        context_block = ""
        
        # Encontrar bloco de contexto
        for i, part in enumerate(dynamic_parts):
            if "Contexto:" in part and i + 1 < len(dynamic_parts):
                context_block = f"{part}\n\n{dynamic_parts[i + 1]}"
                break
        
        # Combinar template com contexto din√¢mico
        merged_parts = []
        
        # 1. Sistema prompt melhorado
        if system_prompt:
            merged_parts.append(system_prompt)
        
        # 2. Template como estrutura principal
        merged_parts.append("# Template de Resposta:")
        merged_parts.append(template_text)
        
        # 3. Contexto din√¢mico
        if context_block:
            merged_parts.append("---")
            merged_parts.append(context_block)
        
        # 4. Query espec√≠fica
        merged_parts.append("---")
        merged_parts.append(f"Query espec√≠fica: {query}")
        
        return "\n\n".join(merged_parts)

    def _apply_task_enhancements(
        self,
        prompt: str,
        task_type: str,
        task_config: Dict[str, Any],
        query: str
    ) -> str:
        """Aplica enhancements espec√≠ficos por tipo de tarefa."""
        
        enhanced_parts = [prompt]
        
        # Enhancement espec√≠fico da tarefa
        system_enhancement = task_config.get("system_enhancement", "")
        if system_enhancement:
            enhanced_parts.append(f"\nüéØ Instru√ß√£o espec√≠fica: {system_enhancement}")
        
        # Adicionar racioc√≠nio se necess√°rio
        if task_config.get("reasoning_required", False):
            if self.dynamic_system._needs_reasoning(query):
                enhanced_parts.append("\nüß† Pense passo a passo antes de responder:")
                enhanced_parts.append("1. Analise o problema")
                enhanced_parts.append("2. Identifique as causas")
                enhanced_parts.append("3. Proponha solu√ß√µes")
                enhanced_parts.append("4. Justifique sua resposta")
        
        # Enhancement por tipo espec√≠fico
        if task_type == "bugfix":
            enhanced_parts.append("\nüêõ Para debugging, inclua:")
            enhanced_parts.append("- Diagn√≥stico da causa raiz")
            enhanced_parts.append("- Reprodu√ß√£o do problema")
            enhanced_parts.append("- Solu√ß√£o com c√≥digo corrigido")
            enhanced_parts.append("- Preven√ß√£o futura")
            
        elif task_type == "review":
            enhanced_parts.append("\nüëÄ Para code review, verifique:")
            enhanced_parts.append("- Legibilidade e manutenibilidade")
            enhanced_parts.append("- Performance e otimiza√ß√µes")
            enhanced_parts.append("- Seguran√ßa e vulnerabilidades")
            enhanced_parts.append("- Testes e cobertura")
            
        elif task_type == "perf":
            enhanced_parts.append("\n‚ö° Para otimiza√ß√£o, analise:")
            enhanced_parts.append("- Gargalos identificados")
            enhanced_parts.append("- M√©tricas de performance")
            enhanced_parts.append("- Solu√ß√µes com impacto mensurado")
            enhanced_parts.append("- Trade-offs das otimiza√ß√µes")
            
        elif task_type == "arch":
            enhanced_parts.append("\nüèóÔ∏è Para arquitetura, considere:")
            enhanced_parts.append("- Requisitos funcionais e n√£o-funcionais")
            enhanced_parts.append("- Padr√µes arquiteturais apropriados")
            enhanced_parts.append("- Escalabilidade e manutenibilidade")
            enhanced_parts.append("- Decis√µes e trade-offs")
            
        elif task_type in ["test", "testgen"]:
            enhanced_parts.append("\nüß™ Para testes, inclua:")
            enhanced_parts.append("- Casos de teste positivos e negativos")
            enhanced_parts.append("- Edge cases e boundary conditions")
            enhanced_parts.append("- Mocks e fixtures necess√°rios")
            enhanced_parts.append("- Estrat√©gia de cobertura")
        
        return "\n".join(enhanced_parts)

    def get_task_suggestions(self, query: str) -> Dict[str, Any]:
        """Retorna sugest√µes de configura√ß√£o baseadas na query."""
        task_type = classify_query(query)
        task_config = self.task_configs.get(task_type, self.task_configs["geral"])
        
        return {
            "task_type": task_type,
            "suggested_temperature": task_config["temperature"],
            "reasoning_recommended": task_config["reasoning_required"],
            "max_context_chunks": task_config["max_context_chunks"],
            "system_enhancement": task_config["system_enhancement"]
        }

    def clear_cache(self):
        """Limpa cache de prompts."""
        self.prompt_cache.clear()
        logger.info("Cache de prompts limpo")

    def get_stats(self) -> Dict[str, Any]:
        """Retorna estat√≠sticas do sistema."""
        return {
            "total_task_types": len(self.task_configs),
            "cache_size": len(self.prompt_cache),
            "available_enhancements": list(self.task_configs.keys())
        }
