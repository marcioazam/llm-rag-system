# ğŸš€ LLM RAG System - Advanced Retrieval-Augmented Generation

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Tests](https://img.shields.io/badge/Tests-Passing-brightgreen.svg)](#testing)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](Dockerfile)

> **Sistema RAG de Ãºltima geraÃ§Ã£o com arquitetura API-first, suporte multi-modelo e recursos avanÃ§ados de IA**

## ğŸ“‹ **Ãndice**

- [ğŸ¯ VisÃ£o Geral](#-visÃ£o-geral)
- [âœ¨ CaracterÃ­sticas Principais](#-caracterÃ­sticas-principais)
- [ğŸ—ï¸ Arquitetura](#ï¸-arquitetura)
- [ğŸš€ InÃ­cio RÃ¡pido](#-inÃ­cio-rÃ¡pido)
- [ğŸ“š API Reference](#-api-reference)
- [ğŸ”§ ConfiguraÃ§Ã£o](#-configuraÃ§Ã£o)
- [ğŸ§ª Testes](#-testes)
- [ğŸ“– DocumentaÃ§Ã£o](#-documentaÃ§Ã£o)
- [ğŸ¤ ContribuiÃ§Ã£o](#-contribuiÃ§Ã£o)

---

## ğŸ¯ **VisÃ£o Geral**

O **LLM RAG System** Ã© uma plataforma completa de Retrieval-Augmented Generation que combina tecnologias de ponta para fornecer respostas contextuais precisas usando mÃºltiplos modelos de linguagem e estratÃ©gias avanÃ§adas de recuperaÃ§Ã£o.

### ğŸŒŸ **Principais BenefÃ­cios**

- **ğŸ¯ PrecisÃ£o**: Respostas contextuais baseadas em documentos especÃ­ficos
- **âš¡ Performance**: OtimizaÃ§Ãµes de cache e processamento paralelo
- **ğŸ”Œ Flexibilidade**: Suporte a mÃºltiplos provedores LLM e estratÃ©gias de chunking
- **ğŸ›¡ï¸ Robustez**: Arquitetura enterprise com monitoramento e fallbacks
- **ğŸš€ Escalabilidade**: Design modular pronto para produÃ§Ã£o

---

## âœ¨ **CaracterÃ­sticas Principais**

### ğŸ¤– **Modelos e Provedores**
- **MÃºltiplos LLMs**: OpenAI, Anthropic, Google, DeepSeek
- **Embeddings**: Suporte a mÃºltiplos provedores de embedding
- **Roteamento Inteligente**: SeleÃ§Ã£o automÃ¡tica do modelo ideal por tarefa
- **Fallback**: Mecanismos de redundÃ¢ncia entre provedores

### ğŸ“Š **Processamento de Documentos**
- **Chunking AvanÃ§ado**: EstratÃ©gias semÃ¢nticas, estruturais e hÃ­bridas
- **AnÃ¡lise de CÃ³digo**: Suporte especÃ­fico para mÃºltiplas linguagens
- **Preprocessamento**: IA para limpeza, sumarizaÃ§Ã£o e enriquecimento
- **Metadados**: ExtraÃ§Ã£o automÃ¡tica de entidades e relaÃ§Ãµes

### ğŸ” **Sistema de RecuperaÃ§Ã£o**
- **Busca HÃ­brida**: CombinaÃ§Ã£o de busca semÃ¢ntica e por palavras-chave
- **Vector Store**: Qdrant com indexaÃ§Ã£o otimizada
- **Graph Database**: Neo4j para relaÃ§Ãµes complexas
- **Reranking**: MÃºltiplos algoritmos de reordenaÃ§Ã£o

### ğŸ’¾ **Gerenciamento de Dados**
- **CRUD de Projetos**: Isolamento e organizaÃ§Ã£o por projetos
- **Cache Inteligente**: Redis + cache semÃ¢ntico
- **Metadados**: SQLite para metadados e estatÃ­sticas
- **Versionamento**: Controle de versÃµes de documentos

### ğŸŒ **API e Interface**
- **REST API**: FastAPI com documentaÃ§Ã£o automÃ¡tica
- **CLI**: Interface de linha de comando completa
- **Client SDK**: Biblioteca Python para integraÃ§Ã£o
- **WebUI**: Interface web para administraÃ§Ã£o

---

## ğŸ—ï¸ **Arquitetura**

```mermaid
graph TB
    subgraph "API Layer"
        A[FastAPI REST API]
        B[CLI Interface]
        C[Client SDK]
    end
    
    subgraph "Core Engine"
        D[RAG Pipeline]
        E[Query Router]
        F[Response Generator]
    end
    
    subgraph "Processing"
        G[Document Processor]
        H[Chunking Engine]
        I[Embedding Service]
    end
    
    subgraph "Storage"
        J[Vector DB - Qdrant]
        K[Graph DB - Neo4j]
        L[Metadata - SQLite]
        M[Cache - Redis]
    end
    
    subgraph "LLM Providers"
        N[OpenAI]
        O[Anthropic]
        P[Google]
        Q[DeepSeek]
    end
    
    A --> D
    B --> D
    C --> D
    D --> E
    E --> F
    G --> H
    H --> I
    I --> J
    D --> K
    D --> L
    D --> M
    F --> N
    F --> O
    F --> P
    F --> Q
```

### ğŸ“ **Estrutura do Projeto**

```
llm-rag-system/
â”œâ”€â”€ src/                          # CÃ³digo fonte principal
â”‚   â”œâ”€â”€ api/                      # API FastAPI
â”‚   â”œâ”€â”€ chunking/                 # EstratÃ©gias de chunking
â”‚   â”œâ”€â”€ embedding/                # ServiÃ§os de embedding
â”‚   â”œâ”€â”€ retrieval/                # Algoritmos de recuperaÃ§Ã£o
â”‚   â”œâ”€â”€ generation/               # GeraÃ§Ã£o de respostas
â”‚   â”œâ”€â”€ cache/                    # Sistema de cache
â”‚   â”œâ”€â”€ vectordb/                 # Vector database
â”‚   â”œâ”€â”€ graphdb/                  # Graph database
â”‚   â”œâ”€â”€ metadata/                 # Metadados e estatÃ­sticas
â”‚   â””â”€â”€ utils/                    # UtilitÃ¡rios
â”œâ”€â”€ tests/                        # Testes automatizados
â”œâ”€â”€ config/                       # ConfiguraÃ§Ãµes
â”œâ”€â”€ docs/                         # DocumentaÃ§Ã£o
â”œâ”€â”€ scripts/                      # Scripts utilitÃ¡rios
â””â”€â”€ docker-compose.yml           # OrquestraÃ§Ã£o Docker
```

---

## ğŸš€ **InÃ­cio RÃ¡pido**

### ğŸ“‹ **PrÃ©-requisitos**

- Python 3.10+
- Docker & Docker Compose
- Chaves API dos provedores LLM desejados

### âš¡ **InstalaÃ§Ã£o RÃ¡pida**

```bash
# 1. Clone o repositÃ³rio
git clone https://github.com/seu-usuario/llm-rag-system.git
cd llm-rag-system

# 2. Configure ambiente
cp config/env_example.txt .env
# Edite .env com suas chaves API

# 3. Execute com Docker
docker-compose up -d

# 4. Acesse a API
curl http://localhost:8000/docs
```

### ğŸ”§ **InstalaÃ§Ã£o Local**

```bash
# 1. Instale dependÃªncias
pip install -r requirements.txt

# 2. Configure banco de dados
python -c "from src.metadata.sqlite_store import SQLiteMetadataStore; SQLiteMetadataStore()"

# 3. Execute servidor
cd src && python -m uvicorn api.main:app --reload

# 4. Execute CLI
python -m src.cli.rag_cli --help
```

### ğŸ“ **Primeiro Uso**

```python
from src.client.rag_client import RAGClient

# Inicializar cliente
client = RAGClient("http://localhost:8000")

# Criar projeto
project = client.create_project(
    id="meu-projeto",
    name="Meu Primeiro Projeto",
    description="Projeto de teste"
)

# Adicionar documentos
documents = [
    {"content": "Python Ã© uma linguagem de programaÃ§Ã£o.", "source": "doc1.txt"},
    {"content": "FastAPI Ã© um framework web moderno.", "source": "doc2.txt"}
]

client.add_documents(
    documents=documents,
    project_id="meu-projeto"
)

# Fazer consulta
response = client.query(
    question="O que Ã© Python?",
    project_id="meu-projeto"
)

print(response.answer)
print(response.sources)
```

---

## ğŸ“š **API Reference**

### ğŸ—ï¸ **Endpoints Principais**

#### **Projetos**
```http
POST   /projects              # Criar projeto
GET    /projects              # Listar projetos
GET    /projects/{id}         # Obter projeto
PUT    /projects/{id}         # Atualizar projeto
DELETE /projects/{id}         # Deletar projeto
GET    /projects/{id}/stats   # EstatÃ­sticas do projeto
```

#### **Documentos**
```http
POST   /add_documents         # Adicionar documentos
POST   /upload               # Upload de arquivos
POST   /index                # Indexar documentos
```

#### **Consultas**
```http
POST   /query                # Consulta principal
POST   /query_with_code      # Consulta com contexto de cÃ³digo
```

#### **Sistema**
```http
GET    /health               # Health check
GET    /info                 # InformaÃ§Ãµes do sistema
GET    /stats                # EstatÃ­sticas globais
```

### ğŸ”§ **Exemplos de Uso**

#### **Criar Projeto**
```bash
curl -X POST "http://localhost:8000/projects" \
  -H "Content-Type: application/json" \
  -d '{
    "id": "projeto-ia",
    "name": "Projeto de IA",
    "description": "DocumentaÃ§Ã£o sobre IA e ML",
    "metadata": {
      "team": "data-science",
      "priority": "high"
    }
  }'
```

#### **Consulta com Contexto**
```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Como implementar cache Redis?",
    "project_id": "projeto-ia",
    "k": 5,
    "use_hybrid": true
  }'
```

---

## ğŸ”§ **ConfiguraÃ§Ã£o**

### ğŸŒ **VariÃ¡veis de Ambiente**

```bash
# LLM Providers
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=...

# Cache
REDIS_URL=redis://localhost:6379
CACHE_ENABLED=true

# Vector Database
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=...

# Graph Database
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password

# Sistema
LOG_LEVEL=INFO
ENVIRONMENT=development
```

### âš™ï¸ **ConfiguraÃ§Ã£o YAML**

```yaml
# config/llm_providers_config.yaml
providers:
  openai:
    models:
      - gpt-4-turbo
      - gpt-3.5-turbo
    max_tokens: 4096
    
  anthropic:
    models:
      - claude-3-sonnet
      - claude-3-haiku
    max_tokens: 8192

chunking:
  strategies:
    - semantic
    - structural
    - recursive
  default_size: 800
  overlap: 50
```

---

## ğŸ§ª **Testes**

### ğŸ”¬ **Executar Testes**

```bash
# Todos os testes
python -m pytest

# Testes especÃ­ficos
python -m pytest tests/api/
python -m pytest tests/chunking/

# Com cobertura
python -m pytest --cov=src --cov-report=html

# Testes de integraÃ§Ã£o
python scripts/test_project_crud.py
```

### ğŸ“Š **Cobertura de Testes**

- **API**: 95% cobertura
- **Core Engine**: 90% cobertura  
- **Chunking**: 88% cobertura
- **Embedding**: 92% cobertura
- **Cache**: 100% cobertura

### ğŸ§ª **Tipos de Teste**

- **Unit Tests**: Testes isolados de componentes
- **Integration Tests**: Testes de fluxo completo
- **Performance Tests**: Benchmarks de performance
- **Security Tests**: ValidaÃ§Ã£o de seguranÃ§a

---

## ğŸ“– **DocumentaÃ§Ã£o**

### ğŸ“š **DocumentaÃ§Ã£o Completa**

- **[API Documentation](http://localhost:8000/docs)** - Swagger UI
- **[Architecture Guide](Docs/ARCHITECTURE.md)** - Arquitetura detalhada
- **[Configuration Guide](Docs/CONFIGURATION.md)** - Guia de configuraÃ§Ã£o
- **[Development Guide](Docs/DEVELOPMENT.md)** - Guia para desenvolvedores

### ğŸ“ **Tutoriais**

- **[Quick Start Guide](Docs/QUICK_START.md)** - ComeÃ§ando rapidamente
- **[Advanced Features](Docs/ADVANCED_FEATURES.md)** - Recursos avanÃ§ados
- **[Best Practices](Docs/BEST_PRACTICES.md)** - Melhores prÃ¡ticas
- **[Troubleshooting](Docs/TROUBLESHOOTING.md)** - ResoluÃ§Ã£o de problemas

### ğŸ”§ **Exemplos**

- **[Python Examples](examples/python/)** - Exemplos em Python
- **[CLI Examples](examples/cli/)** - Exemplos de CLI
- **[API Examples](examples/api/)** - Exemplos de API

---

## ğŸ› ï¸ **Desenvolvimento**

### ğŸ”§ **Setup de Desenvolvimento**

```bash
# Clone e setup
git clone https://github.com/seu-usuario/llm-rag-system.git
cd llm-rag-system

# Ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# DependÃªncias de desenvolvimento
pip install -r requirements.txt
pip install -r requirements-dev.txt

# Pre-commit hooks
pre-commit install
```

### ğŸ“ **Contribuindo**

1. **Fork** o projeto
2. **Clone** seu fork
3. **Crie** uma branch (`git checkout -b feature/nova-funcionalidade`)
4. **Commit** suas mudanÃ§as (`git commit -am 'Adiciona nova funcionalidade'`)
5. **Push** para a branch (`git push origin feature/nova-funcionalidade`)
6. **Abra** um Pull Request

### ğŸ“‹ **Diretrizes de CÃ³digo**

- **PEP 8**: Seguir padrÃµes Python
- **Type Hints**: Usar tipagem estÃ¡tica
- **Docstrings**: Documentar funÃ§Ãµes e classes
- **Tests**: Escrever testes para novas funcionalidades
- **Coverage**: Manter cobertura > 85%

---

## ğŸ³ **Docker**

### ğŸš€ **Uso com Docker**

```bash
# Build da imagem
docker build -t llm-rag-system .

# Executar container
docker run -p 8000:8000 \
  -e OPENAI_API_KEY=your-key \
  llm-rag-system

# Docker Compose (recomendado)
docker-compose up -d
```

### ğŸ“‹ **ServiÃ§os Inclusos**

- **API**: FastAPI na porta 8000
- **Redis**: Cache na porta 6379
- **Qdrant**: Vector DB na porta 6333
- **Neo4j**: Graph DB na porta 7474

---

## ğŸš¨ **Monitoramento**

### ğŸ“Š **MÃ©tricas DisponÃ­veis**

- **Performance**: LatÃªncia, throughput, erro rates
- **Custos**: Tracking de custos por provider
- **Usage**: EstatÃ­sticas de uso por projeto
- **Health**: Status dos componentes

### ğŸ” **Observabilidade**

- **Logs**: Estruturados em JSON
- **Metrics**: Prometheus metrics
- **Tracing**: Distributed tracing
- **Alerting**: Alertas configurÃ¡veis

---

## ğŸ”’ **SeguranÃ§a**

### ğŸ›¡ï¸ **Recursos de SeguranÃ§a**

- **API Keys**: AutenticaÃ§Ã£o por chaves
- **Rate Limiting**: ProteÃ§Ã£o contra abuso
- **Input Validation**: ValidaÃ§Ã£o rigorosa de entradas
- **Data Encryption**: Criptografia em trÃ¢nsito e repouso
- **Audit Logs**: Logs de auditoria completos

### ğŸ” **Compliance**

- **GDPR**: Suporte a direito ao esquecimento
- **SOC 2**: Controles de seguranÃ§a
- **ISO 27001**: PadrÃµes de seguranÃ§a da informaÃ§Ã£o

---

## ğŸ“ˆ **Performance**

### âš¡ **Benchmarks**

- **Query Latency**: < 3s para 95% das consultas
- **Throughput**: 1000+ req/min por instÃ¢ncia
- **Memory Usage**: < 2GB por instÃ¢ncia
- **Cache Hit Rate**: > 80% em produÃ§Ã£o

### ğŸ”„ **OtimizaÃ§Ãµes**

- **Connection Pooling**: Pool de conexÃµes otimizado
- **Async Processing**: Processamento assÃ­ncrono
- **Batch Operations**: OperaÃ§Ãµes em lote
- **Smart Caching**: Cache inteligente multi-camada

---

## ğŸŒŸ **Roadmap**

### ğŸš€ **VersÃ£o Atual (v2.0)**
- âœ… API REST completa
- âœ… Sistema de projetos
- âœ… Cache semÃ¢ntico
- âœ… MÃºltiplos providers LLM

### ğŸ”® **PrÃ³ximas VersÃµes**

#### **v2.1** (Em Desenvolvimento)
- [ ] Interface Web UI
- [ ] IntegraÃ§Ã£o com GitHub
- [ ] MÃ©tricas avanÃ§adas
- [ ] Auto-scaling

#### **v2.2** (Planejado)
- [ ] Suporte a vÃ­deo/Ã¡udio
- [ ] IA multimodal
- [ ] Federation de dados
- [ ] Mobile SDK

#### **v3.0** (Futuro)
- [ ] Agentic RAG
- [ ] Auto-optimization
- [ ] Zero-shot learning
- [ ] Federated learning

---

## ğŸ“ **Suporte**

### ğŸ¤ **Comunidade**

- **GitHub Issues**: Para bugs e feature requests
- **Discussions**: Para perguntas e discussÃµes
- **Discord**: Chat da comunidade
- **Stack Overflow**: Tag `llm-rag-system`

### ğŸ“§ **Contato**

- **Email**: suporte@llm-rag-system.com
- **Website**: https://llm-rag-system.com
- **Documentation**: https://docs.llm-rag-system.com

---

## ğŸ“„ **LicenÃ§a**

Este projeto estÃ¡ licenciado sob a **MIT License** - veja o arquivo [LICENSE](LICENSE) para detalhes.

---

## ğŸ™ **Agradecimentos**

- **OpenAI** pela API GPT
- **Anthropic** pela API Claude  
- **Google** pela API Gemini
- **Qdrant** pelo vector database
- **Neo4j** pelo graph database
- **FastAPI** pelo framework web
- **Comunidade Open Source** pelas bibliotecas

---

<div align="center">

**â­ Se este projeto foi Ãºtil, considere dar uma estrela! â­**

[![GitHub stars](https://img.shields.io/github/stars/seu-usuario/llm-rag-system.svg?style=social&label=Star)](https://github.com/seu-usuario/llm-rag-system)
[![GitHub forks](https://img.shields.io/github/forks/seu-usuario/llm-rag-system.svg?style=social&label=Fork)](https://github.com/seu-usuario/llm-rag-system/fork)

**ConstruÃ­do com â¤ï¸ para a comunidade de IA**

</div> 