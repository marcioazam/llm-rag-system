# ğŸš€ Sistema RAG com APIs Externas

Sistema RAG (Retrieval-Augmented Generation) totalmente baseado em APIs de provedores externos como OpenAI, Anthropic, Google e Groq. **Sem dependÃªncias de modelos locais.**

## âœ¨ CaracterÃ­sticas Principais

### ğŸ¯ **Roteamento Inteligente de Modelos**
- **GPT-4o**: AnÃ¡lise complexa, arquitetura de sistemas, code reviews
- **GPT-4o-mini**: GeraÃ§Ã£o de cÃ³digo, debugging, testes unitÃ¡rios, refatoraÃ§Ã£o
- **Claude 3.5 Sonnet**: AnÃ¡lise de documentos, criaÃ§Ã£o de conteÃºdo, escrita tÃ©cnica
- **Claude 3 Haiku**: Respostas rÃ¡pidas, extraÃ§Ã£o de dados, classificaÃ§Ã£o
- **Gemini 1.5 Pro**: AnÃ¡lise multimodal, raciocÃ­nio de contexto longo

### ğŸ’° **Controle Inteligente de Custos**
- Monitoramento de custos em tempo real
- Cache inteligente para reduzir chamadas de API
- SeleÃ§Ã£o automÃ¡tica do modelo mais econÃ´mico por tarefa
- Limites configurÃ¡veis de orÃ§amento diÃ¡rio
- Fallback automÃ¡tico entre provedores

### âš¡ **Performance Otimizada**
- Cache de embeddings e respostas
- Batching automÃ¡tico de requests
- Rate limiting configurÃ¡vel
- Retry automÃ¡tico com backoff exponencial

## ğŸ› ï¸ ConfiguraÃ§Ã£o RÃ¡pida

### 1. **Instalar DependÃªncias**
```bash
pip install -r requirements.txt
```

### 2. **Configurar API Keys**
```bash
# Copiar arquivo de exemplo
cp config/env_example.txt .env

# Editar com suas API keys
nano .env
```

**API Keys necessÃ¡rias (mÃ­nimo OPENAI_API_KEY):**
```bash
OPENAI_API_KEY=sk-your-openai-key-here
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here  # Recomendado
GOOGLE_API_KEY=your-google-key-here               # Opcional
GROQ_API_KEY=gsk_your-groq-key-here               # Opcional
```

### 3. **Executar MigraÃ§Ã£o**
```bash
python migrate_to_api_system.py
```

### 4. **Testar Sistema**
```python
from src.rag_pipeline_api import APIRAGPipeline

# Inicializar pipeline
pipeline = APIRAGPipeline()

# Verificar saÃºde do sistema
health = pipeline.health_check()
print(f"Status: {health['status']}")
print(f"API Keys configuradas: {health['api_keys_configured']}")
```

## ğŸ“š Uso BÃ¡sico

### **Adicionar Documentos**
```python
# Documentos para indexar
documents = [
    {
        "content": "Python Ã© uma linguagem de programaÃ§Ã£o de alto nÃ­vel...",
        "metadata": {
            "source": "tutorial_python.md",
            "type": "documentation",
            "language": "pt"
        }
    },
    {
        "content": "FastAPI Ã© um framework web moderno para Python...",
        "metadata": {
            "source": "fastapi_guide.md",
            "type": "tutorial"
        }
    }
]

# Adicionar ao Ã­ndice
result = pipeline.add_documents(documents)
print(f"âœ… {result['documents_processed']} documentos processados")
print(f"ğŸ“„ {result['chunks_created']} chunks criados")
```

### **Fazer Consultas**
```python
# Query simples
response = pipeline.query("Como criar uma API com FastAPI?")
print(f"Resposta: {response['answer']}")
print(f"Modelo usado: {response['model_used']} ({response['provider_used']})")
print(f"Custo: ${response['cost']:.4f}")

# Query com parÃ¢metros especÃ­ficos
from src.models.api_model_router import TaskType

response = pipeline.query(
    question="Crie uma funÃ§Ã£o Python para calcular fibonacci",
    k=3,  # NÃºmero de chunks para recuperar
    task_type=TaskType.CODE_GENERATION,  # ForÃ§ar tipo de tarefa
    force_model="openai.gpt4o_mini",     # ForÃ§ar modelo especÃ­fico
    include_sources=True                  # Incluir fontes na resposta
)

print(f"CÃ³digo gerado: {response['answer']}")
print(f"Fontes utilizadas: {len(response['sources'])}")
```

### **Monitorar EstatÃ­sticas**
```python
stats = pipeline.get_stats()
print(f"Total de queries: {stats['total_queries']}")
print(f"Custo total: ${stats['total_cost']:.4f}")
print(f"Documentos indexados: {stats['total_documents_indexed']}")
print(f"Uso por provedor: {stats['provider_usage']}")
```

## ğŸ¯ Responsabilidades dos Modelos

### **OpenAI GPT-4o** 
- âœ… RaciocÃ­nio complexo e anÃ¡lise crÃ­tica
- âœ… Arquitetura de sistemas e design patterns  
- âœ… Code review e anÃ¡lise de qualidade
- âœ… ResoluÃ§Ã£o de problemas complexos

### **OpenAI GPT-4o-mini**
- ğŸ”§ GeraÃ§Ã£o de cÃ³digo eficiente
- ğŸ› Debugging e correÃ§Ã£o de erros
- ğŸ§ª CriaÃ§Ã£o de testes unitÃ¡rios
- â™»ï¸ RefatoraÃ§Ã£o de cÃ³digo

### **Claude 3.5 Sonnet**
- ğŸ“– AnÃ¡lise profunda de documentos
- âœï¸ CriaÃ§Ã£o de conteÃºdo tÃ©cnico
- ğŸ”¬ SÃ­ntese de pesquisas
- ğŸ“ DocumentaÃ§Ã£o tÃ©cnica

### **Claude 3 Haiku**
- âš¡ Respostas rÃ¡pidas e diretas
- ğŸ“Š ExtraÃ§Ã£o de dados estruturados
- ğŸ·ï¸ ClassificaÃ§Ã£o de conteÃºdo
- ğŸ” Tarefas simples e objetivas

## âš™ï¸ ConfiguraÃ§Ãµes AvanÃ§adas

### **Configurar Limites de Custo**
```yaml
# config/llm_providers_config.yaml
routing:
  cost_limits:
    daily_budget: 50.00          # OrÃ§amento diÃ¡rio em USD
    per_request_limit: 1.00      # Limite por request
    warn_threshold: 0.80         # Alertar com 80% do orÃ§amento
```

### **EstratÃ©gias de Roteamento**
```yaml
routing:
  strategy: "cost_performance_optimized"  # OpÃ§Ãµes:
  # - cost_optimized: Sempre o modelo mais barato
  # - performance_optimized: Sempre o melhor modelo
  # - balanced: EquilÃ­brio entre custo e qualidade
  # - cost_performance_optimized: OtimizaÃ§Ã£o inteligente
```

### **Configurar Cache**
```yaml
optimization:
  caching:
    enabled: true
    ttl_seconds: 7200            # Cache por 2 horas
    max_cache_size: 2000         # MÃ¡ximo 2000 entradas
    cache_embeddings: true       # Cache de embeddings
    cache_responses: true        # Cache de respostas
```

## ğŸŒ API REST

### **Iniciar Servidor**
```bash
python main.py
# Servidor disponÃ­vel em http://localhost:8000
```

### **Endpoints Principais**

#### **GET /** - InformaÃ§Ãµes da API
```bash
curl http://localhost:8000/
```

#### **GET /health** - Verificar SaÃºde
```bash
curl http://localhost:8000/health
```

#### **POST /query** - Consulta RAG
```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Como criar uma API com FastAPI?",
    "k": 5,
    "include_sources": true
  }'
```

#### **POST /documents** - Adicionar Documentos
```bash
curl -X POST http://localhost:8000/documents \
  -H "Content-Type: application/json" \
  -d '{
    "documents": [
      {
        "content": "ConteÃºdo do documento...",
        "metadata": {"source": "exemplo.md"}
      }
    ]
  }'
```

#### **GET /stats** - EstatÃ­sticas
```bash
curl http://localhost:8000/stats
```

#### **GET /models** - Modelos DisponÃ­veis
```bash
curl http://localhost:8000/models
```

## ğŸ’¡ Dicas de OtimizaÃ§Ã£o

### **Reduzir Custos**
```python
# 1. Use cache agressivamente
pipeline.config["optimization"]["caching"]["ttl_seconds"] = 86400  # 24h

# 2. Prefira modelos menores para tarefas simples
response = pipeline.query(
    "Qual a capital do Brasil?",
    force_model="openai.gpt35_turbo"  # Modelo mais barato
)

# 3. Configure orÃ§amento diÃ¡rio
# Edite config/llm_providers_config.yaml
```

### **Melhorar Performance**
```python
# 1. Use batching para mÃºltiplas queries
queries = ["Pergunta 1", "Pergunta 2", "Pergunta 3"]
responses = [pipeline.query(q) for q in queries]

# 2. Configure chunking apropriado
from src.chunking.recursive_chunker import RecursiveChunker
chunker = RecursiveChunker(
    chunk_size=750,      # Chunks maiores para menos calls
    chunk_overlap=75     # Overlap adequado
)

# 3. Use embeddings otimizados
response = pipeline.query(
    question="Sua pergunta",
    k=3  # Menos chunks = menos custo
)
```

## ğŸ” Troubleshooting

### **Erro: API Key nÃ£o configurada**
```bash
# Verificar se .env existe e tem as keys
cat .env | grep API_KEY

# Verificar se variÃ¡veis estÃ£o sendo carregadas
python -c "import os; print(os.getenv('OPENAI_API_KEY', 'NOT_FOUND'))"
```

### **Erro: Limite de rate excedido**
```python
# Configurar rate limiting
config["optimization"]["rate_limiting"]["requests_per_minute"] = 50
```

### **Custos muito altos**
```python
# Verificar estatÃ­sticas
stats = pipeline.get_stats()
print(f"Custo total: ${stats['total_cost']:.2f}")

# Configurar cache mais agressivo
config["optimization"]["caching"]["ttl_seconds"] = 86400  # 24h
```

## ğŸ“Š Monitoramento

### **MÃ©tricas Principais**
- **Total de requests**: NÃºmero total de chamadas
- **Custo total**: Gasto acumulado em USD
- **Cache hit rate**: EficiÃªncia do cache
- **Tempo mÃ©dio de resposta**: Performance
- **DistribuiÃ§Ã£o de provedores**: Uso por API

### **Alertas ConfigurÃ¡veis**
- **Alto custo**: Alerta quando atingir 80% do orÃ§amento
- **Alta latÃªncia**: Alerta para respostas > 60s
- **Taxa de erro**: Alerta para erro > 5%

## ğŸ¤ Contribuindo

1. Fork o repositÃ³rio
2. Crie uma branch para sua feature
3. Implemente suas mudanÃ§as
4. Adicione testes se necessÃ¡rio
5. Submeta um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo LICENSE para detalhes.

---

## ğŸ‰ BenefÃ­cios do Sistema

âœ… **Zero Infraestrutura Local**: NÃ£o precisa de GPUs, CUDA ou modelos grandes  
âœ… **Escalabilidade AutomÃ¡tica**: APIs escalam conforme demanda  
âœ… **Modelos Estado-da-Arte**: Sempre os melhores modelos disponÃ­veis  
âœ… **Custo Controlado**: Pague apenas pelo que usar com limites configurÃ¡veis  
âœ… **Alta Disponibilidade**: MÃºltiplos provedores com fallback automÃ¡tico  
âœ… **FÃ¡cil ManutenÃ§Ã£o**: Sem updates de modelos ou gerenciamento de infra  

**Seu sistema RAG agora usa a melhor IA disponÃ­vel no mercado! ğŸš€** 